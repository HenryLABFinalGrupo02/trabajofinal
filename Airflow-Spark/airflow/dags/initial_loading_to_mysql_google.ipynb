{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INSTALLATION OF LIBRARIES\n",
    "!sudo apt update\n",
    "!sudo apt install openjdk-17-jdk -y\n",
    "#!curl -JLO 'https://apache.osuosl.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz'\n",
    "!tar xvf spark-3.3.1-bin-hadoop3.tgz\n",
    "!mv spark-3.3.1-bin-hadoop3 /opt/spark\n",
    "!pip install pyspark missingno findspark plotly wandb koalas pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LIBRARIES\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "#### BASIC\n",
    "import pandas as pd\n",
    "\n",
    "#### SPARK\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "import sqlalchemy\n",
    "ps.set_option('compute.ops_on_diff_frames', True)\n",
    "from transform_funcs import *\n",
    "#from tiny_functions import *\n",
    "\n",
    "def lower_col_names(cols):\n",
    "    new_names = {}\n",
    "    for x in cols:\n",
    "        new_names[x] = x.lower()\n",
    "    return new_names    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_len(value):\n",
    "  ls=value.split(', ')\n",
    "  return len(ls)\n",
    "\n",
    "\n",
    "def dicc(row):\n",
    "    #result = json.loads(row)\n",
    "    result = ast.literal_eval(row)\n",
    "    return result\n",
    "\n",
    "def etl_atributtes(business):\n",
    "    atributtes = pd.json_normalize(data = business['attributes'])\n",
    "    atributtes['BusinessParking'].fillna(\"{'garage': False, 'street': False, 'validated': False, 'lot': False, 'valet': False}\", inplace=True)\n",
    "    atributtes.loc[atributtes['BusinessParking'] == 'None', 'BusinessParking'] = \"{'garage': False, 'street': False, 'validated': False, 'lot': False, 'valet': False}\"\n",
    "\n",
    "    atributtes.loc[atributtes['Ambience'] == 'None', 'Ambience'] = \"{'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': False}\"\n",
    "    atributtes['Ambience'] = atributtes['Ambience'].fillna(\"{'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': False}\")\n",
    "\n",
    "    \n",
    "\n",
    "    garage = pd.json_normalize(atributtes.BusinessParking.apply(dicc))\n",
    "    ambience = pd.json_normalize(atributtes['Ambience'].apply(dicc))\n",
    "\n",
    "    atributtes['garage'] = garage['garage']\n",
    "    atributtes['garage'].fillna(0, inplace=True)\n",
    "\n",
    "    atributtes.drop(['BusinessParking', 'Ambience'], axis=1, inplace=True)\n",
    "\n",
    "    ambience['good_ambience'] = ambience['romantic'] + ambience['intimate'] + ambience['classy'] + ambience['hipster'] + ambience['touristy'] + ambience['trendy'] + ambience['upscale'] + ambience['casual'] + ambience['divey']\n",
    "\n",
    "    ambience.loc[ambience['good_ambience'] > 1, 'good_ambience'] = 1\n",
    "    ambience.fillna(0, inplace=True)\n",
    "    atributtes['good_ambience'] = ambience['good_ambience']\n",
    "\n",
    "    atributtes['RestaurantsTakeOut'].fillna(0, inplace=True)\n",
    "    atributtes['RestaurantsDelivery'].fillna(0, inplace=True)\n",
    "    atributtes['RestaurantsTakeOut'] = atributtes['RestaurantsTakeOut'].map({'True': 1, 'False': 0, 'None': 0})\n",
    "    atributtes['RestaurantsDelivery'] = atributtes['RestaurantsDelivery'].map({'True': 1, 'False': 0, 'None': 0})\n",
    "    atributtes['delivery'] = atributtes['RestaurantsTakeOut'] + atributtes['RestaurantsDelivery']\n",
    "    atributtes['delivery'] = pd.to_numeric(atributtes['delivery'], errors='coerce').fillna(0).astype(int)\n",
    "    atributtes['delivery'] = atributtes['delivery'].replace(to_replace=2, value=1)\n",
    "\n",
    "\n",
    "    atributtes.drop(['RestaurantsTakeOut', 'RestaurantsDelivery'], axis=1, inplace=True)\n",
    "\n",
    "    top20 = atributtes.notna().sum().sort_values(ascending=False).head(20).index.tolist()\n",
    "\n",
    "    atributtes = atributtes[top20]\n",
    "\n",
    "    atributtes.fillna(0, inplace=True)\n",
    "    atributtes.replace('None', 0, inplace=True)\n",
    "    atributtes.replace('False', 0, inplace=True)\n",
    "    atributtes.replace(False, 0, inplace=True)\n",
    "    atributtes.replace('True', 1, inplace=True)\n",
    "\n",
    "    atributtes['WiFi'] = np.where(atributtes['WiFi'] != 0, 1, 0)\n",
    "\n",
    "    atributtes.loc[atributtes['Alcohol'] == \"u'none'\", 'Alcohol'] = 0\n",
    "    atributtes.loc[atributtes['Alcohol'] == \"'none'\", 'Alcohol'] = 0\n",
    "    atributtes.loc[atributtes['Alcohol'] != 0, 'Alcohol'] = 1\n",
    "    atributtes['Alcohol'].unique()\n",
    "\n",
    "    atributtes.loc[atributtes['NoiseLevel'].str.contains('quiet').fillna(False), 'NoiseLevel'] = 1\n",
    "    atributtes.loc[atributtes['NoiseLevel'].str.contains('average').fillna(False), 'NoiseLevel'] = 2\n",
    "    atributtes.loc[atributtes['NoiseLevel'].str.contains('loud').fillna(False), 'NoiseLevel'] = 3\n",
    "    atributtes.loc[atributtes['RestaurantsAttire'] != 0,'RestaurantsAttire'] = 1\n",
    "    atributtes.loc[atributtes['garage'] != 0,'garage'] = 1\n",
    "    atributtes.loc[atributtes['GoodForMeal'] == 0,'GoodForMeal'] = \"{'dessert': False, 'latenight': False, 'lunch': False, 'dinner': False, 'brunch': False, 'breakfast': False}\"\n",
    "    atributtes['GoodForMeal'].fillna(\"{'dessert': False, 'latenight': False, 'lunch': False, 'dinner': False, 'brunch': False, 'breakfast': False}\", inplace=True)\n",
    "    meal_types = pd.json_normalize(atributtes['GoodForMeal'].apply(dicc))\n",
    "    atributtes.drop(['GoodForMeal'], axis=1, inplace=True)\n",
    "    atributtes['meal_diversity'] = meal_types['dessert'] + meal_types['latenight'] + meal_types['lunch'] + meal_types['dinner'] + meal_types['brunch'] + meal_types['breakfast']\n",
    "    atributtes['meal_diversity'].fillna(0, inplace=True)\n",
    "    atributtes['business_id'] = business['business_id']\n",
    "    return atributtes\n",
    "\n",
    "def open_time(x):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    else:\n",
    "        return x.split('-')[0]\n",
    "\n",
    "def close_time(x):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    else:\n",
    "        return x.split('-')[1]\n",
    "\n",
    "\n",
    "def normalize_hours(lista:list, df):\n",
    "    for i in lista:\n",
    "        df.loc[pd.notnull(df[i]), i] = pd.to_datetime(df.loc[pd.notnull(df[i]), i]) - pd.to_datetime(df.loc[pd.notnull(df[i]), i]).dt.normalize()\n",
    "\n",
    "def mean_open_hour(lista:list, df):\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        temp_sum = []\n",
    "        for j in lista:\n",
    "            if pd.notnull(df.loc[i, j]):\n",
    "                temp_sum.append(df.loc[i, j])\n",
    "        if len(temp_sum) > 0:\n",
    "            df.loc[i, 'mean_open_hour'] = sum(temp_sum,datetime.timedelta())/len(temp_sum)\n",
    "\n",
    "def mean_close_hour(lista:list, df):\n",
    "    for i in range(df.shape[0]):\n",
    "        temp_sum = []\n",
    "        for j in lista:\n",
    "            if pd.notnull(df.loc[i, j]):\n",
    "                temp_sum.append(df.loc[i, j])\n",
    "        if len(temp_sum) > 0:\n",
    "            df.loc[i, 'mean_close_hour'] = sum(temp_sum,datetime.timedelta())/len(temp_sum)\n",
    "\n",
    "\n",
    "def etl_hours(business):\n",
    "    hours = pd.json_normalize(data = business['hours'])\n",
    "    hours['business_id'] = business['business_id']\n",
    "    subset = hours[~pd.isnull(hours[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']]).any(1)].index\n",
    "    hours['7days'] = 0\n",
    "    hours.loc[subset, '7days'] = 1\n",
    "    subset = hours[~pd.isnull(hours[['Friday', 'Saturday', 'Sunday']]).any(1)].index\n",
    "    hours['weekends'] = 0\n",
    "    hours.loc[subset, 'weekends'] = 1\n",
    "\n",
    "    hours['monday_open_time'] = pd.to_datetime(hours['Monday'].apply(open_time), format='%H:%M')\n",
    "    hours['monday_close_time'] = pd.to_datetime(hours['Monday'].apply(close_time), format='%H:%M')\n",
    "    hours['monday_total_hours'] = (hours['monday_close_time'] - hours['monday_open_time'])/pd.Timedelta(hours=1)\n",
    "    hours.loc[hours['monday_total_hours'] == 0.0, 'monday_total_hours'] = 24.0\n",
    "\n",
    "    hours['tuesday_open_time'] = pd.to_datetime(hours['Tuesday'].apply(open_time), format='%H:%M')\n",
    "    hours['tuesday_close_time'] = pd.to_datetime(hours['Tuesday'].apply(close_time), format='%H:%M')\n",
    "    hours['tuesday_total_hours'] = (hours['tuesday_close_time'] - hours['tuesday_open_time'])/pd.Timedelta(hours=1)\n",
    "    hours.loc[hours['tuesday_total_hours'] == 0.0, 'tuesday_total_hours'] = 24.0\n",
    "\n",
    "\n",
    "    hours['wednesday_open_time'] = pd.to_datetime(hours['Wednesday'].apply(open_time), format='%H:%M')\n",
    "    hours['wednesday_close_time'] = pd.to_datetime(hours['Wednesday'].apply(close_time), format='%H:%M')\n",
    "    hours['wednesday_total_hours'] = (hours['wednesday_close_time'] - hours['wednesday_open_time'])/pd.Timedelta(hours=1)\n",
    "    hours.loc[hours['wednesday_total_hours'] == 0.0, 'wednesday_total_hours'] = 24.0\n",
    "\n",
    "\n",
    "    hours['thursday_open_time'] = pd.to_datetime(hours['Thursday'].apply(open_time), format='%H:%M')\n",
    "    hours['thursday_close_time'] = pd.to_datetime(hours['Thursday'].apply(close_time), format='%H:%M')\n",
    "    hours['thursday_total_hours'] = (hours['thursday_close_time'] - hours['thursday_open_time'])/pd.Timedelta(hours=1)\n",
    "    hours.loc[hours['thursday_total_hours'] == 0.0, 'thursday_total_hours'] = 24.0\n",
    "\n",
    "\n",
    "    hours['friday_open_time'] = pd.to_datetime(hours['Friday'].apply(open_time), format='%H:%M')\n",
    "    hours['friday_close_time'] = pd.to_datetime(hours['Friday'].apply(close_time), format='%H:%M')\n",
    "    hours['friday_total_hours'] = (hours['friday_close_time'] - hours['friday_open_time'])/pd.Timedelta(hours=1)\n",
    "    hours.loc[hours['friday_total_hours'] == 0.0, 'friday_total_hours'] = 24.0\n",
    "\n",
    "\n",
    "    hours['saturday_open_time'] = pd.to_datetime(hours['Saturday'].apply(open_time), format='%H:%M')\n",
    "    hours['saturday_close_time'] = pd.to_datetime(hours['Saturday'].apply(close_time), format='%H:%M')\n",
    "    hours['saturday_total_hours'] = (hours['saturday_close_time'] - hours['saturday_open_time'])/pd.Timedelta(hours=1)\n",
    "    hours.loc[hours['saturday_total_hours'] == 0.0, 'saturday_total_hours'] = 24.0\n",
    "\n",
    "\n",
    "    hours['sunday_open_time'] = pd.to_datetime(hours['Sunday'].apply(open_time), format='%H:%M')\n",
    "    hours['sunday_close_time'] = pd.to_datetime(hours['Sunday'].apply(close_time), format='%H:%M')\n",
    "    hours['sunday_total_hours'] = (hours['sunday_close_time'] - hours['sunday_open_time'])/pd.Timedelta(hours=1)\n",
    "    hours.loc[hours['sunday_total_hours'] == 0.0, 'sunday_total_hours'] = 24.0\n",
    "\n",
    "    lista_total_open_hours = ['monday_total_hours', 'tuesday_total_hours', 'wednesday_total_hours', 'thursday_total_hours', 'friday_total_hours', 'saturday_total_hours', 'sunday_total_hours']\n",
    "    lista_open = ['monday_open_time', 'tuesday_open_time', 'wednesday_open_time', 'thursday_open_time', 'friday_open_time', 'saturday_open_time', 'sunday_open_time']\n",
    "    lista_close = ['monday_close_time', 'tuesday_close_time', 'wednesday_close_time', 'thursday_close_time', 'friday_close_time', 'saturday_close_time', 'sunday_close_time']\n",
    "    lista_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    lista_total =  lista_total_open_hours + lista_open + lista_close + lista_days\n",
    "\n",
    "    hours['n_open_days'] = pd.notnull(hours[lista_open]).sum(axis=1)\n",
    "    hours['mean_total_hours_open'] = hours[lista_total_open_hours].abs().mean(axis=1)\n",
    "\n",
    "    normalize_hours(lista_open, hours)\n",
    "    normalize_hours(lista_close, hours)\n",
    "    mean_open_hour(lista_open, hours)\n",
    "    mean_close_hour(lista_close, hours)\n",
    "    hours.drop(lista_total, axis=1, inplace=True)\n",
    "    return hours\n",
    "\n",
    "def etl_categories(business):\n",
    "    categories = business['categories'].str.split(', ', expand=True)\n",
    "    categories.index = business['business_id']\n",
    "    df = categories.T.stack().groupby('business_id').apply(list).reset_index(name='categories')\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df = df.join(pd.DataFrame(mlb.fit_transform(df.pop('categories')),\n",
    "                            columns=mlb.classes_,\n",
    "                            index=df.index))\n",
    "    df['total_categories'] = df.sum(axis=1)\n",
    "    most_frequent = ['Restaurants',\n",
    "    'Food',\n",
    "    'Shopping',\n",
    "    'Home Services',\n",
    "    'Beauty & Spas',\n",
    "    'Nightlife',\n",
    "    'Health & Medical',\n",
    "    'Local Services',\n",
    "    'Bars',\n",
    "    'Automotive', 'total_categories', 'business_id']\n",
    "    df = df.fillna(0)\n",
    "    df = df[most_frequent]\n",
    "    return df\n",
    "\n",
    "def etl_gps(business):\n",
    "    for_clustering = business[['latitude', 'longitude', 'business_id']]\n",
    "    kmeans = KMeans(n_clusters = 11, init ='k-means++')\n",
    "    Y_axis = for_clustering['latitude'].values.reshape(-1,1)\n",
    "    X_axis = for_clustering['longitude'].values.reshape(-1,1)\n",
    "    for_clustering.loc[:,'areas'] = kmeans.fit_predict(X_axis, Y_axis)\n",
    "    return for_clustering\n",
    "\n",
    "\n",
    "def get_len(value):\n",
    "  \"\"\"\n",
    "  It takes a string of comma separated names and returns the number of names in the string\n",
    "  \n",
    "  :param value: the value of the column you're applying the function to\n",
    "  :return: The number of friends in the list.\n",
    "  \"\"\"\n",
    "  ls = value.split(', ')\n",
    "  return len(ls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_bad_str(input):\n",
    "    if pd.isna(input):\n",
    "        return 'NO DATA'\n",
    "    else:\n",
    "        output = input.strip().replace('  ',' ').lower()\n",
    "        if len(output) <=2:\n",
    "            return 'REMOVE_THIS_ROW'\n",
    "        else: return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "def transform_dates(input):\n",
    "    if isinstance(input,str):\n",
    "        try:\n",
    "            return(dateutil.parser.parse(input))  #NO REEMPLAZA POR MODA | USAR APPLY\n",
    "        except: return pd.NaT\n",
    "    else: return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING TIPS FILE\n",
      "DROPPING DUPLICATED ROWS\n",
      "CALCULATING TOTAL CHECKINS\n",
      "CONNECTING TO DATABASE and UPLOADING\n",
      "   COUNT(*)\n",
      "0    131930\n"
     ]
    }
   ],
   "source": [
    "def load_n_checkin():\n",
    "    #### READS FILE AND MAKES TRANSFORMATION\n",
    "    print('READING TIPS FILE')\n",
    "    checkin = pd.read_json('./data/checkin.json', lines=True)\n",
    "\n",
    "    #### TRANSFORMATIONS\n",
    "    print('DROPPING DUPLICATED ROWS')\n",
    "    checkin = checkin.drop_duplicates()\n",
    "\n",
    "    print(\"CALCULATING TOTAL CHECKINS\")\n",
    "    checkin['total'] = checkin['date'].apply(lambda x: get_len(x))\n",
    "\n",
    "    checkin.drop('date', axis=1, inplace=True)\n",
    "\n",
    "    checkin.rename(columns=lower_col_names(checkin.columns), inplace=True)\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "    \n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "                .format(user=\"root\",\n",
    "                        address = '35.239.80.227:3306',\n",
    "                        pw=\"Henry12.BORIS99\",\n",
    "                        db=\"yelp\"))\n",
    "\n",
    "    checkin.to_sql('n_checkins', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM n_checkins;\", con=engine).head())\n",
    "\n",
    "load_n_checkin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING TIPS FILE\n",
      "DROPPING DUPLICATED ROWS\n",
      "CLEANING STRINGS\n",
      "NORMALIZING DATES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/extension.py:101: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  return Index(result, name=self.name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTING TO DATABASE and UPLOADING\n",
      "   COUNT(*)\n",
      "0    899854\n"
     ]
    }
   ],
   "source": [
    "def load_tips():\n",
    "    #### READS FILE AND MAKES TRANSFORMATION\n",
    "    print('READING TIPS FILE')\n",
    "    tip = pd.read_json('./data/tip_big/tip_big.json',lines=True)\n",
    "\n",
    "    #### TRANSFORMATIONS\n",
    "    print('DROPPING DUPLICATED ROWS')\n",
    "    tip = tip.drop_duplicates()\n",
    "    print('CLEANING STRINGS')\n",
    "    tip['text'] = tip['text'].apply(lambda x: drop_bad_str(x))\n",
    "\n",
    "    print('NORMALIZING DATES')\n",
    "    tip['date'] = tip['date'].apply(lambda x: transform_dates(x)).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    tip.rename(columns=lower_col_names(tip.columns), inplace=True)\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "    \n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "                .format(user=\"root\",\n",
    "                        address = '35.239.80.227:3306',\n",
    "                        pw=\"Henry12.BORIS99\",\n",
    "                        db=\"yelp\"))\n",
    "\n",
    "    tip.to_sql('tip', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM tip;\", con=engine).head())\n",
    "\n",
    "load_tips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING TIPS FILE\n",
      "CONNECTING TO DATABASE and UPLOADING\n",
      "   COUNT(*)\n",
      "0    105376\n"
     ]
    }
   ],
   "source": [
    "def load_top_tips():\n",
    "    ##### UPLOADS SMALL DATASET TOP TIPS TO CASSANDRA\n",
    "    #### READS FILE AND MAKES TRANSFORMATION\n",
    "    print('READING TIPS FILE')\n",
    "    tip = pd.read_json('./data/tip_big/tip_big.json',lines=True)\n",
    "\n",
    "    #### MAKES PANDAS TRANSFORMATION TO GET TOP TIPS\n",
    "    top_tips = pd.DataFrame(tip['business_id'].value_counts())\n",
    "    top_tips.rename(columns={'business_id': 'number_tips'}, inplace=True)\n",
    "    top_tips['business_id'] = top_tips.index\n",
    "    top_tips.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    top_tips.rename(columns=lower_col_names(top_tips.columns), inplace=True)\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "                .format(user=\"root\",\n",
    "                        address = '35.239.80.227:3306',\n",
    "                        pw=\"Henry12.BORIS99\",\n",
    "                        db=\"yelp\"))\n",
    "\n",
    "    top_tips.to_sql('top_tip', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM top_tip;\", con=engine).head())\n",
    "\n",
    "load_top_tips()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING BUSINESS FILE\n",
      "TRANSFORMING ATTRIBUTES\n",
      "TRANSFORMING HOURS\n",
      "TRANSFORMING CATEGORIES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3867/1686793685.py:196: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df['total_categories'] = df.sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPS CLUSTERING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3867/1686793685.py:216: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  for_clustering.loc[:,'areas'] = kmeans.fit_predict(X_axis, Y_axis)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXING RestaurantsPriceRange2 AND DELIVERY\n",
      "[0 1]\n",
      "MERGING DATAFRAMES\n",
      "DONE\n",
      "CONNECTING TO DATABASE and UPLOADING\n",
      "   COUNT(*)\n",
      "0    150346\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def load_business():\n",
    "    print('READING BUSINESS FILE')\n",
    "    business = pd.read_json('./data/business.json', lines=True)\n",
    "    \n",
    "\n",
    "    print('TRANSFORMING ATTRIBUTES')\n",
    "    atributtes = etl_atributtes(business)\n",
    "\n",
    "    print('TRANSFORMING HOURS')\n",
    "    hours = etl_hours(business)\n",
    "\n",
    "    print('TRANSFORMING CATEGORIES') #FILLLED NA\n",
    "    categories = etl_categories(business)\n",
    "\n",
    "    print('GPS CLUSTERING')\n",
    "    gps = etl_gps(business)\n",
    "\n",
    "    print('FIXING RestaurantsPriceRange2 AND DELIVERY')\n",
    "    atributtes['RestaurantsPriceRange2'] = pd.to_numeric(atributtes['RestaurantsPriceRange2'])\n",
    "    \n",
    "    print(atributtes['delivery'].unique())\n",
    "\n",
    "    print('MERGING DATAFRAMES')\n",
    "    data_frames = [business, atributtes, categories, hours, gps]\n",
    "    full_data = reduce(lambda left,right: pd.merge(left,right,on='business_id', how='left'), data_frames)\n",
    "    full_data = full_data.drop(['attributes', 'hours', 'city', 'state', 'categories', 'latitude_y', 'longitude_y'], axis=1)\n",
    "\n",
    "    full_data.rename(columns={'Home Services':'HomeServices','Beauty & Spas':'BeautyAndSpas', 'Health & Medical':'HealthAndMedical','Local Services':'LocalServices', '7days':'SevenDays'}, inplace=True)\n",
    "\n",
    "    full_data['mean_open_hour'] = full_data.mean_open_hour.astype(str)\n",
    "    full_data['mean_close_hour'] = full_data.mean_close_hour.astype(str)\n",
    "    full_data['RestaurantsPriceRange2'] = full_data.RestaurantsPriceRange2.astype(str)\n",
    "\n",
    "    full_data.rename(columns=lower_col_names(full_data.columns), inplace=True)\n",
    "\n",
    "    print('DONE')\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "                .format(user=\"root\",\n",
    "                        address = '35.239.80.227:3306',\n",
    "                        pw=\"Henry12.BORIS99\",\n",
    "                        db=\"yelp\"))\n",
    "\n",
    "    full_data.to_sql('business_clean', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM business_clean;\", con=engine).head())\n",
    "\n",
    "\n",
    "load_business()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING USER FILE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "NORMALIZING DATES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING ELITE & FRIENDS\n",
      "USER COLS\n",
      "Index(['average_stars', 'compliment_cool', 'compliment_cute',\n",
      "       'compliment_funny', 'compliment_hot', 'compliment_list',\n",
      "       'compliment_more', 'compliment_note', 'compliment_photos',\n",
      "       'compliment_plain', 'compliment_profile', 'compliment_writer', 'cool',\n",
      "       'fans', 'funny', 'name', 'review_count', 'useful', 'user_id',\n",
      "       'yelping_since'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTING MYSQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING PART 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING PART 200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING PART 400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING PART 800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING PART 1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING PART 1200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING PART 1400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING PART 1600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPLOADING PART 1800000\n",
      "   COUNT(*)\n",
      "0    150346\n"
     ]
    }
   ],
   "source": [
    "def load_user():\n",
    "    print('READING USER FILE')\n",
    "    user = ps.read_json('./data/user.json', lines=True)\n",
    "    print('DROPPING DUPLICATED ROWS')\n",
    "    user = user.drop_duplicates()\n",
    "\n",
    "    print('NORMALIZING DATES')\n",
    "    user['yelping_since'] = user['yelping_since'].apply(transform_dates).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    print('DROPPING ELITE & FRIENDS')\n",
    "    user = user.drop(['friends', 'elite'], axis=1)\n",
    "\n",
    "    print('USER COLS')\n",
    "    print(user.columns)\n",
    "\n",
    "    user.rename(columns=lower_col_names(user.columns), inplace=True)\n",
    "\n",
    "    \n",
    "    l1 = [0,200000, 400000, 800000, 1000000,1200000, 1400000, 1600000, 1800000]\n",
    "    l2 = [200000, 400000, 800000, 1000000,1200000, 1400000, 1600000, 1800000, user.shape[0]]\n",
    "\n",
    "    print('CONNECTING MYSQL')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "        .format(user=\"root\",\n",
    "                address = '35.239.80.227:3306',\n",
    "                pw=\"Henry12.BORIS99\",\n",
    "                db=\"yelp\"))\n",
    "\n",
    "    for i,j in list(zip(l1,l2)):        \n",
    "        df = user.iloc[i:j, :].to_pandas()\n",
    "        print('UPLOADING PART {}'.format(i))\n",
    "        df.to_sql('user', con=engine, if_exists='append', index=False)\n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM business_clean;\", con=engine).head())\n",
    "\n",
    "load_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_metrics(): \n",
    "    \"\"\"\n",
    "    The function takes a dataframe of users and returns a dataframe with the influencer score for each\n",
    "    user\n",
    "\n",
    "    :param user: the user dataframe\n",
    "    :return: A dataframe with the columns: n_interactions_received, n_interactions_send, fans,\n",
    "    friends_number, Score_influencer, Influencer, user_id\n",
    "    \"\"\"\n",
    "    print('READING USER FILE')\n",
    "    user = ps.read_json('./data/user.json', lines=True)\n",
    "    print('DROPPING DUPLICATED ROWS')\n",
    "    user = user.drop_duplicates()\n",
    "\n",
    "    print('GENERATING INTERACTIONS RECIEVED COLUMN')\n",
    "    user['n_ints_rec'] = user[[ 'compliment_hot',\n",
    "    'compliment_more', 'compliment_profile', 'compliment_cute',\n",
    "    'compliment_list', 'compliment_note', 'compliment_plain',\n",
    "    'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
    "    'compliment_photos']].sum(axis=1)\n",
    "    print('GENERATING INTERACTIONS SEND COLUMN')\n",
    "    user['n_interactions_send'] = user['useful'] + user['funny'] + user['cool']\n",
    "    print('GENERATING FRIENDS NUM COLUMN')\n",
    "    user['friends_number'] = user.friends.apply(get_len)\n",
    "    print('GENERATING INFLUENCER COLUMN')\n",
    "    user['Influencer'] = user['n_ints_rec'] / (1 + user['friends_number'] + user['fans'])\n",
    "    user['Influencer'].fillna(0, inplace = True)\n",
    "    print('GENERATING INFLUENCER SCORE COLUMN')\n",
    "    user['Influencer_Score'] = 1 - (1 / (1 + user['Influencer']))\n",
    "    print('GENERATING INFLUENCER 2 COLUMN')\n",
    "    user['Influencer_2'] = user['n_ints_rec'] / (1 + user['fans'])\n",
    "    user['Influencer_2'].fillna(0, inplace = True)\n",
    "    print('GENERATING INFLUENCER SCORE 2 COLUMN')\n",
    "    user['Influencer_Score_2'] = 1 - (1 / (1 + user['Influencer_2']))\n",
    "\n",
    "    print('GENERATING NEW DF')\n",
    "    user = user[['user_id', 'n_ints_rec', 'n_interactions_send', 'fans', 'friends_number',\n",
    "    'Influencer', 'Influencer_Score', 'Influencer_2', 'Influencer_Score_2']]\n",
    "    print('TRANSFORMATIONS FOR USER METRICS DONE')\n",
    "\n",
    "    user.rename(columns=lower_col_names(user.columns), inplace=True)\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "\n",
    "    l1 = [0,200000, 400000, 800000, 1000000,1200000, 1400000, 1600000, 1800000]\n",
    "    l2 = [200000, 400000, 800000, 1000000,1200000, 1400000, 1600000, 1800000, user.shape[0]]\n",
    "\n",
    "    print('CONNECTING MYSQL')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "        .format(user=\"root\",\n",
    "                address = '35.239.80.227:3306',\n",
    "                pw=\"Henry12.BORIS99\",\n",
    "                db=\"yelp\"))\n",
    "\n",
    "    for i,j in list(zip(l1,l2)):        \n",
    "        df = user.iloc[i:j, :].to_pandas()\n",
    "        print('UPLOADING PART {}'.format(i))\n",
    "        df.to_sql('user_metrics', con=engine, if_exists='append', index=False)\n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM user_metrics;\", con=engine).head())\n",
    "\n",
    "load_user_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING USER FILE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "GENERATING INTERACTIONS RECIEVED COLUMN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING INTERACTIONS SEND COLUMN\n",
      "GENERATING FRIENDS NUM COLUMN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING INFLUENCER COLUMN\n",
      "GENERATING INFLUENCER SCORE COLUMN\n",
      "GENERATING INFLUENCER 2 COLUMN\n",
      "GENERATING INFLUENCER SCORE 2 COLUMN\n",
      "GENERATING NEW DF\n",
      "TRANSFORMATIONS FOR USER METRICS DONE\n",
      "CONNECTING TO DATABASE and UPLOADING\n",
      "CONNECTING MYSQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   COUNT(*)\n",
      "0   1987897\n"
     ]
    }
   ],
   "source": [
    "def load_user_metrics(): \n",
    "    \"\"\"\n",
    "    The function takes a dataframe of users and returns a dataframe with the influencer score for each\n",
    "    user\n",
    "\n",
    "    :param user: the user dataframe\n",
    "    :return: A dataframe with the columns: n_interactions_received, n_interactions_send, fans,\n",
    "    friends_number, Score_influencer, Influencer, user_id\n",
    "    \"\"\"\n",
    "    print('READING USER FILE')\n",
    "    user = ps.read_json('./data/user.json', lines=True).iloc[1800000:, :]\n",
    "    print('DROPPING DUPLICATED ROWS')\n",
    "    user = user.drop_duplicates()\n",
    "\n",
    "    print('GENERATING INTERACTIONS RECIEVED COLUMN')\n",
    "    user['n_ints_rec'] = user[[ 'compliment_hot',\n",
    "    'compliment_more', 'compliment_profile', 'compliment_cute',\n",
    "    'compliment_list', 'compliment_note', 'compliment_plain',\n",
    "    'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
    "    'compliment_photos']].sum(axis=1)\n",
    "    print('GENERATING INTERACTIONS SEND COLUMN')\n",
    "    user['n_interactions_send'] = user['useful'] + user['funny'] + user['cool']\n",
    "    print('GENERATING FRIENDS NUM COLUMN')\n",
    "    user['friends_number'] = user.friends.apply(get_len)\n",
    "    print('GENERATING INFLUENCER COLUMN')\n",
    "    user['Influencer'] = user['n_ints_rec'] / (1 + user['friends_number'] + user['fans'])\n",
    "    user['Influencer'].fillna(0, inplace = True)\n",
    "    print('GENERATING INFLUENCER SCORE COLUMN')\n",
    "    user['Influencer_Score'] = 1 - (1 / (1 + user['Influencer']))\n",
    "    print('GENERATING INFLUENCER 2 COLUMN')\n",
    "    user['Influencer_2'] = user['n_ints_rec'] / (1 + user['fans'])\n",
    "    user['Influencer_2'].fillna(0, inplace = True)\n",
    "    print('GENERATING INFLUENCER SCORE 2 COLUMN')\n",
    "    user['Influencer_Score_2'] = 1 - (1 / (1 + user['Influencer_2']))\n",
    "\n",
    "    print('GENERATING NEW DF')\n",
    "    user = user[['user_id', 'n_ints_rec', 'n_interactions_send', 'fans', 'friends_number',\n",
    "    'Influencer', 'Influencer_Score', 'Influencer_2', 'Influencer_Score_2']]\n",
    "    print('TRANSFORMATIONS FOR USER METRICS DONE')\n",
    "\n",
    "    user.rename(columns=lower_col_names(user.columns), inplace=True)\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "\n",
    "    print('CONNECTING MYSQL')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "        .format(user=\"root\",\n",
    "                address = '35.239.80.227:3306',\n",
    "                pw=\"Henry12.BORIS99\",\n",
    "                db=\"yelp\"))\n",
    "\n",
    "    user.to_pandas().to_sql('user_metrics', con=engine, if_exists='append', index=False)\n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM user_metrics;\", con=engine).head())\n",
    "\n",
    "load_user_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTING TO DATABASE and UPLOADING\n",
      "READING REVIEW FILE 0+100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 0100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 100000+200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 100000200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 200000+300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 200000300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 300000+400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 300000400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 400000+500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 400000500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 500000+600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 500000600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 600000+700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 600000700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 700000+800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 700000800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 800000+900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 800000900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 900000+1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 9000001000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 1000000+1100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 10000001100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 1100000+1200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 11000001200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 1200000+1300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 12000001300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 1300000+1400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 13000001400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 1400000+1500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 14000001500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 1500000+1600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 15000001600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 1600000+1700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 16000001700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 1700000+1800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 17000001800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 1800000+1900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 18000001900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 1900000+2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 19000002000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 2000000+2100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 20000002100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 2100000+2200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 21000002200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 2200000+2300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 22000002300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 2300000+2400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 23000002400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 2400000+2500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 24000002500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 2500000+2600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 25000002600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 2600000+2700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 26000002700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 2700000+2800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 27000002800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 2800000+2900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 28000002900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 2900000+3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 29000003000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 3000000+3100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 30000003100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 3100000+3200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 31000003200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 3200000+3300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 32000003300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 3300000+3400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 33000003400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 3400000+3500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 34000003500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 3500000+3600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 35000003600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 3600000+3700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 36000003700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 3700000+3800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 37000003800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 3800000+3900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 38000003900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 3900000+4000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 39000004000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 4000000+4100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 40000004100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 4100000+4200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 41000004200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 4200000+4300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 42000004300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 4300000+4400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 43000004400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 4400000+4500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 44000004500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 4500000+4600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 45000004600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 4600000+4700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 46000004700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 4700000+4800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 47000004800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 4800000+4900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 48000004900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 4900000+5000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 49000005000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 5000000+5100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 50000005100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 5100000+5200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 51000005200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 5200000+5300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 52000005300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 5300000+5400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 53000005400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 5400000+5500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 54000005500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 5500000+5600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 55000005600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 5600000+5700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 56000005700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 5700000+5800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 57000005800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 5800000+5900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 58000005900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 5900000+6000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 59000006000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 6000000+6100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 60000006100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 6100000+6200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 61000006200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 6200000+6300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 62000006300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 6300000+6400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 63000006400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 6400000+6500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 64000006500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING REVIEW FILE 6500000+6815608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_json`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING DUPLICATED ROWS\n",
      "UPLOADING PART 65000006815608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   COUNT(*)\n",
      "0   6815608\n"
     ]
    }
   ],
   "source": [
    "def load_review():\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "    .format(user=\"root\",\n",
    "            address = '35.239.80.227:3306',\n",
    "            pw=\"Henry12.BORIS99\",\n",
    "            db=\"yelp\"))\n",
    "\n",
    "    lista1 = []\n",
    "    lista2 = []\n",
    "    for i in range(0,6600000, 100000):\n",
    "        lista1.append(i)\n",
    "\n",
    "    lista2 = lista1[1:]\n",
    "    lista2.append(6815608)\n",
    "\n",
    "    splitter = list(zip(lista1,lista2))\n",
    "\n",
    "    #print(splitter)\n",
    "    for i,j in splitter:        \n",
    "\n",
    "        print('READING REVIEW FILE {}+{}'.format(i,j))\n",
    "        review = ps.read_json('./data/review_big/review_big.json').iloc[i:j, :]\n",
    "        \n",
    "        print('DROPPING DUPLICATED ROWS')\n",
    "        review = review.drop_duplicates()\n",
    "\n",
    "        #print('NORMALIZING DATES')\n",
    "        #review['date'] = review['date'].apply(transform_dates).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        review.rename(columns=lower_col_names(review.columns), inplace=True)\n",
    "\n",
    "        print('UPLOADING PART {}{}'.format(i,j))\n",
    "        review.to_pandas().to_sql('review', con=engine, if_exists='append', index=False)\n",
    "        \n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM review;\", con=engine).head())\n",
    "    \n",
    "\n",
    "load_review()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymysql\n",
      "Successfully installed pymysql-1.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTING TO DATABASE and UPLOADING\n",
      "READING FILE\n",
      "DROPPING DUPLICATED ROWS\n",
      "   COUNT(*)\n",
      "0    150346\n"
     ]
    }
   ],
   "source": [
    "def load_sentiment_by_business():\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "    .format(user=\"root\",\n",
    "            address = '35.239.80.227:3306',\n",
    "            pw=\"Henry12.BORIS99\",\n",
    "            db=\"yelp\"))\n",
    "\n",
    "\n",
    "    print('READING FILE')\n",
    "    sentiment = pd.read_csv('./data/sentiment/sentiment_ok_unique.csv')\n",
    "\n",
    "    print('DROPPING DUPLICATED ROWS')\n",
    "    sentiment = sentiment.drop_duplicates()\n",
    "\n",
    "    sentiment.to_sql('sentiment_by_business', con=engine, if_exists='append', index=False)\n",
    "        \n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM sentiment_by_business;\", con=engine).head())\n",
    "    \n",
    "\n",
    "load_sentiment_by_business()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTING TO DATABASE and UPLOADING\n",
      "READING FILE:  ./data/sentiment/ok/dataset_1.csv\n",
      "LOADING TO DATABASE\n",
      "READING FILE:  ./data/sentiment/ok/dataset_3.csv\n",
      "LOADING TO DATABASE\n",
      "READING FILE:  ./data/sentiment/ok/dataset_7.csv\n",
      "LOADING TO DATABASE\n",
      "READING FILE:  ./data/sentiment/ok/dataset_5.csv\n",
      "LOADING TO DATABASE\n",
      "READING FILE:  ./data/sentiment/ok/dataset_4.csv\n",
      "LOADING TO DATABASE\n",
      "READING FILE:  ./data/sentiment/ok/dataset_2.csv\n",
      "LOADING TO DATABASE\n",
      "READING FILE:  ./data/sentiment/ok/dataset_6.csv\n",
      "LOADING TO DATABASE\n",
      "   COUNT(*)\n",
      "0   6990280\n"
     ]
    }
   ],
   "source": [
    "def load_sentiment_by_review():\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "    .format(user=\"root\",\n",
    "            address = '35.239.80.227:3306',\n",
    "            pw=\"Henry12.BORIS99\",\n",
    "            db=\"yelp\"))\n",
    "\n",
    "    path = './data/sentiment/ok/'\n",
    "    all_csv = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    for i in all_csv:\n",
    "        print('READING FILE: ', i)\n",
    "        sentiment = pd.read_csv(i)\n",
    "        print('LOADING TO DATABASE')\n",
    "        sentiment.to_sql('sentiment_by_review', con=engine, if_exists='append', index=False)\n",
    "        \n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM sentiment_by_review;\", con=engine).head())\n",
    "    \n",
    "\n",
    "load_sentiment_by_review()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTING TO DATABASE and UPLOADING\n",
      "READING FILE\n",
      "DROPPING DUPLICATED ROWS\n",
      "   COUNT(*)\n",
      "0    133399\n"
     ]
    }
   ],
   "source": [
    "def load_review_timeseries():\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "    .format(user=\"root\",\n",
    "            address = '35.239.80.227:3306',\n",
    "            pw=\"Henry12.BORIS99\",\n",
    "            db=\"yelp\"))\n",
    "\n",
    "\n",
    "    print('READING FILE')\n",
    "    review_ts = pd.read_csv('./data/review_timeseries.csv')\n",
    "\n",
    "    print('DROPPING DUPLICATED ROWS')\n",
    "    review_ts = review_ts.drop_duplicates()\n",
    "\n",
    "    review_ts.to_sql('review_timeseries_top_brands', con=engine, if_exists='append', index=False)\n",
    "        \n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM review_timeseries_top_brands;\", con=engine).head())\n",
    "    \n",
    "\n",
    "load_review_timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTING TO DATABASE and UPLOADING\n",
      "READING FILE\n",
      "DROPPING DUPLICATED ROWS\n",
      "   COUNT(*)\n",
      "0     24519\n"
     ]
    }
   ],
   "source": [
    "def load_tip_timeseries():\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "    .format(user=\"root\",\n",
    "            address = '35.239.80.227:3306',\n",
    "            pw=\"Henry12.BORIS99\",\n",
    "            db=\"yelp\"))\n",
    "\n",
    "\n",
    "    print('READING FILE')\n",
    "    tip_ts = pd.read_csv('./data/tip_timeseries.csv')\n",
    "\n",
    "    print('DROPPING DUPLICATED ROWS')\n",
    "    tip_ts = tip_ts.drop_duplicates()\n",
    "\n",
    "    tip_ts.to_sql('tip_timeseries_top_brands', con=engine, if_exists='append', index=False)\n",
    "        \n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM tip_timeseries_top_brands;\", con=engine).head())\n",
    "    \n",
    "\n",
    "load_tip_timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTING TO DATABASE and UPLOADING\n",
      "READING FILE\n",
      "DROPPING DUPLICATED ROWS\n",
      "   COUNT(*)\n",
      "0    500150\n"
     ]
    }
   ],
   "source": [
    "def load_checkin_timeseries():\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "    .format(user=\"root\",\n",
    "            address = '35.239.80.227:3306',\n",
    "            pw=\"Henry12.BORIS99\",\n",
    "            db=\"yelp\"))\n",
    "\n",
    "\n",
    "    print('READING FILE')\n",
    "    checkin_ts = pd.read_csv('./data/checkin_timeseries.csv')\n",
    "\n",
    "    print('DROPPING DUPLICATED ROWS')\n",
    "    checkin_ts = checkin_ts.drop_duplicates()\n",
    "\n",
    "    checkin_ts.to_sql('checkin_timeseries_top_brands', con=engine, if_exists='append', index=False)\n",
    "        \n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM checkin_timeseries_top_brands;\", con=engine).head())\n",
    "    \n",
    "\n",
    "load_checkin_timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTING TO DATABASE and UPLOADING\n",
      "READING FILE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32/419694690.py:12: DtypeWarning: Columns (35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  attributes_ts = pd.read_csv('./data/attributes.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   COUNT(*)\n",
      "0    150346\n"
     ]
    }
   ],
   "source": [
    "def load_attributes():\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "    .format(user=\"root\",\n",
    "            address = '35.239.80.227:3306',\n",
    "            pw=\"Henry12.BORIS99\",\n",
    "            db=\"yelp\"))\n",
    "\n",
    "\n",
    "    print('READING FILE')\n",
    "    attributes_ts = pd.read_csv('./data/attributes.csv', low_memory=False)\n",
    "\n",
    "    #print('DROPPING DUPLICATED ROWS')\n",
    "    #attributes_ts = attributes_ts.drop_duplicates()\n",
    "\n",
    "    attributes_ts.to_sql('attributes_business', con=engine, if_exists='append', index=False)\n",
    "        \n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM attributes_business;\", con=engine).head())\n",
    "    \n",
    "\n",
    "load_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTING TO DATABASE and UPLOADING\n",
      "READING FILE\n",
      "   COUNT(*)\n",
      "0    150346\n"
     ]
    }
   ],
   "source": [
    "def load_categories():\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "    .format(user=\"root\",\n",
    "            address = '35.239.80.227:3306',\n",
    "            pw=\"Henry12.BORIS99\",\n",
    "            db=\"yelp\"))\n",
    "\n",
    "    print('READING FILE')\n",
    "    business = pd.read_json('./data/business.json', lines=True)\n",
    "    categories_ts = business[['business_id','categories']]\n",
    "\n",
    "    #print('DROPPING DUPLICATED ROWS')\n",
    "    #categories_ts = categories_ts.drop_duplicates()\n",
    "\n",
    "    categories_ts.to_sql('categories_business', con=engine, if_exists='append', index=False)\n",
    "        \n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM categories_business;\", con=engine).head())\n",
    "    \n",
    "\n",
    "load_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str('mWMc6_wTdE0EUBKIGXDVfA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DROP TABLE FUNCTION for MySQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import MetaData\n",
    "# from sqlalchemy import create_engine\n",
    "# from sqlalchemy.engine.url import URL\n",
    "# from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "# engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "# .format(user=\"root\",\n",
    "#         address = '35.239.80.227:3306',\n",
    "#         pw=\"Henry12.BORIS99\",\n",
    "#         db=\"yelp\"))\n",
    "# def drop_table(table_name, engine=engine):\n",
    "#     Base = declarative_base()\n",
    "#     metadata = MetaData()\n",
    "#     metadata.reflect(bind=engine)\n",
    "#     table = metadata.tables[table_name]\n",
    "#     if table is not None:\n",
    "#         Base.metadata.drop_all(engine, [table], checkfirst=True)\n",
    "\n",
    "# drop_table('categories_business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTING TO DATABASE and UPLOADING\n",
      "READING FILE\n",
      "   COUNT(*)\n",
      "0    150346\n"
     ]
    }
   ],
   "source": [
    "def load_business_target():\n",
    "\n",
    "    print('CONNECTING TO DATABASE and UPLOADING')\n",
    "    engine = sqlalchemy.create_engine(\"mysql+pymysql://{user}:{pw}@{address}/{db}\"\n",
    "    .format(user=\"root\",\n",
    "            address = '35.239.80.227:3306',\n",
    "            pw=\"Henry12.BORIS99\",\n",
    "            db=\"yelp\"))\n",
    "\n",
    "\n",
    "    print('READING FILE')\n",
    "    business_target = pd.read_csv('../../../ML/data/target_3_influencer_modified.csv', low_memory=False)\n",
    "\n",
    "    #print('DROPPING DUPLICATED ROWS')\n",
    "    #attributes_ts = attributes_ts.drop_duplicates()\n",
    "\n",
    "    business_target.to_sql('business_target', con=engine, if_exists='append', index=False)\n",
    "        \n",
    "    print(pd.read_sql(\"SELECT COUNT(*) FROM business_target;\", con=engine).head())\n",
    "    \n",
    "\n",
    "load_business_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b31da22c807bd73dc1165632752c731a8c1e75a303b1219ebd41ecef46a6dd35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
