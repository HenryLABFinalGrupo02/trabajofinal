{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/23 19:21:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/11/23 19:21:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "######## LIBRARIES IMPORT ##########\n",
    "####################################\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import databricks.koalas as ks\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "####################################\n",
    "######## SPARK RUNNING ##########\n",
    "####################################\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/23 19:21:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "######## LIBRARIES IMPORT ##########\n",
    "####################################\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import databricks.koalas as ks\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "####################################\n",
    "######## SPARK RUNNING ##########\n",
    "####################################\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "'''\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('SparkCassandraApp') \\\n",
    "    .config('spark.cassandra.connection.host', 'localhost') \\\n",
    "    .config('spark.cassandra.connection.port', '9042') \\\n",
    "    .config('spark.cassandra.output.consistency.level','ONE') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "'''\n",
    "ks.set_option('compute.ops_on_diff_frames', True)\n",
    "\n",
    "####################################\n",
    "######## PATH SETTINGS ##########\n",
    "####################################\n",
    "path_1 = \"./\"\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "\n",
    "def values_type(dataframe, column): \n",
    "    \"\"\"\n",
    "    This functions returns a set that contains the data types contained within a column of a pandas or koalas dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: pandas or koalas dataframe\n",
    "    - column: the name of the column to analize\n",
    "    \"\"\"\n",
    "    types = set()\n",
    "    for value in dataframe[column].to_numpy():\n",
    "        types.add(type(value))\n",
    "    return types\n",
    "\n",
    "# DEALING WITH DUPLICATED REGISTERS\n",
    "\n",
    "def drop_duplicates(Table):\n",
    "    \"\"\"\n",
    "    Returns a Dataframe with no duplicates\n",
    "\n",
    "    Parameters:\n",
    "    - Table: Pandas or Koalas dataframe\n",
    "    \"\"\"\n",
    "    return Table.drop_duplicates()\n",
    "\n",
    "\n",
    "####################################\n",
    "######## COMMON FUNCTIONS ##########\n",
    "####################################\n",
    "def import_json(file:str, path:Path = path_1, format:str = 'json'):\n",
    "    '''\n",
    "    This function imports files with spark and transforms them into DataFrame using the koala library\n",
    "\n",
    "    Arguments:\n",
    "    :: file: str of the file name\n",
    "    :: path: 'path' path where the file is stored\n",
    "    :: format: 'str' file format \n",
    "\n",
    "    Returns: \n",
    "    ---------\n",
    "    Dataframe and print shape \n",
    "    '''\n",
    "    path_final = path + file\n",
    "    print('READING JSON')\n",
    "    df = ks.read_json(path_final, lines=True)\n",
    "    print(f\"Shape of {file} is {df.shape}\")\n",
    "    return df\n",
    "    \n",
    "\n",
    "def upload_to_cassandra(df, table_name):\n",
    "    df.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=table_name, keyspace=\"yelp\")\\\n",
    "    .mode('append')\\\n",
    "    .save()\n",
    "\n",
    "\n",
    "# ID VALIDATION\n",
    "\n",
    "def check_id_chars(Table, id_column):\n",
    "    \"\"\"\n",
    "    Checks if the strings in an ID column have the required characters (20).\n",
    "    This function is meant to be called within the 'drop_bad_ids' function.\n",
    "    Returns a list of indexes at which the column has an invalid ID.\n",
    "\n",
    "    Parameters:\n",
    "    - Table: Pandas or Koalas dataframe\n",
    "    - id_column: column containing 22 character ID's\n",
    "    \"\"\"\n",
    "    problems = []\n",
    "    for index, value in Table[id_column].items() :\n",
    "        if len(value) != 22:\n",
    "            problems.append(index)\n",
    "    return problems\n",
    "\n",
    "def drop_bad_ids(Table, id_column):\n",
    "    \"\"\"\n",
    "    This function removes the rows in a table where an ID is not valid.\n",
    "    Returns a table with only valid ID's in the passed column.\n",
    "\n",
    "    Parameters:\n",
    "    - Table: Koalas dataframe\n",
    "    - id_column: column containing 22 character ID's\n",
    "    \"\"\"\n",
    "    id_list = check_id_chars(Table, id_column)\n",
    "    return Table[ks.Series((~Table.index.isin(id_list)).to_list())].reset_index(drop=True)\n",
    "\n",
    "# NUMERIC VALUES\n",
    "\n",
    "def impute_num(Table, col_list, absolute=False):\n",
    "    \"\"\"\n",
    "    This function replaces missing values in numeric columns with 0.\n",
    "    If the 'absolute' parameter is passed, the function also converts \n",
    "    the numeric columns into their absolute value.\n",
    "\n",
    "    Parameters:\n",
    "    - Table: Pandas or Koalas dataframe\n",
    "    - col_list: list of numeric columns with missing values to be imputed\n",
    "    - absolute: boolean, decides if the column will contain absolute values. Default: False.\n",
    "    \"\"\"\n",
    "    for col in col_list:\n",
    "        Table[col].fillna(0)\n",
    "        if absolute:\n",
    "            Table[col] = Table[col].apply(lambda x: abs(x))\n",
    "\n",
    "# STRING VALUES\n",
    "\n",
    "def clean_string(string):\n",
    "    \"\"\"\n",
    "    This function cleans strings by removing whitespaces at the beginning and \n",
    "    at the end of the string, replacing double spaces with single spaces and\n",
    "    converting the string to lower case.\n",
    "    It is meant to be used within the 'drop_bad_str' function.\n",
    "    Returns a clean string.\n",
    "\n",
    "    Parameters:\n",
    "    - string: some string to be cleaned\n",
    "    \"\"\"\n",
    "    new_str = string.strip().replace('  ',' ').lower()\n",
    "    return new_str\n",
    "\n",
    "def drop_bad_str(Table, col):\n",
    "    \"\"\"\n",
    "    This function takes a Dataframe and the name of a column that contains string values, \n",
    "    imputes missing values in the column, cleans it's strings and removes registers where \n",
    "    the string in the column has 2 or less characters.\n",
    "    The function returns the dataframe after performing the above mentioned transformations\n",
    "    and dropping the unwanted registers.\n",
    "\n",
    "    Parameters:\n",
    "    - Table: Pandas or Koalas dataframe\n",
    "    - col: string, the name of the column to transform\n",
    "    \"\"\"\n",
    "    T_ok = Table.copy()\n",
    "    T_ok[col] = T_ok[col].fillna('NO DATA')\n",
    "    T_ok[col] = T_ok[col].apply(clean_string)\n",
    "    bad_strs = []\n",
    "    for index, tip in T_ok[col].items():\n",
    "        if len(tip) <=2:\n",
    "            bad_strs.append(index)\n",
    "    return T_ok[ks.Series((~Table.index.isin(bad_strs)).to_list())].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# DATETIME VALUES\n",
    "\n",
    "def transform_dates(dataframe,column,format):\n",
    "    \"\"\"\n",
    "    This function recieves 1) a dataframe, 2) the name of a column containing timestamp values\n",
    "    and 3) a date format. It returns the dataframe after transforming the column to the desired \n",
    "    format.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: a Koalas dataframe\n",
    "    - column: the name of the column containing timestamp values\n",
    "    - format: the datetime format to which the column will be transformed\n",
    "    \"\"\"\n",
    "    series = ks.to_datetime(dataframe[column], errors='coerce')\n",
    "    mode = series.mode().iloc[0].strftime(format)\n",
    "    series = series.apply(lambda x: mode if (x is pd.NaT) else x.strftime(format))\n",
    "    return series\n",
    "\n",
    "# LISTS OF STRINGS\n",
    "\n",
    "def check_str_list(ls):\n",
    "    \"\"\"\n",
    "    This function recieves a list and returns a second list containing only the strings from the\n",
    "    original list. In case there were none, it returns an empty list. If a None value is passed, \n",
    "    the function returns an empty list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ls_ok = []\n",
    "        for x in ls:\n",
    "            if type(x) == str:\n",
    "                ls_ok.append(x)\n",
    "        return ls_ok\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# DICTIONARY\n",
    "\n",
    "def row_hours_to_list(row):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, each sublist containing the day of the week, it's opening hour and it's closing hour. E.g.: [[1,8,18],[2,8,18]...]\n",
    "\n",
    "    Parameters:\n",
    "    - row: pyspark row object\n",
    "    \"\"\"\n",
    "    dicc = row.asDict()\n",
    "    day_dicc = {\n",
    "        'Monday': 1,\n",
    "        'Tuesday': 2,\n",
    "        'Wednesday': 3,\n",
    "        'Thursday': 4,\n",
    "        'Friday': 5,\n",
    "        'Saturday': 6,\n",
    "        'Sunday': 7\n",
    "    }\n",
    "\n",
    "    check = zip(dicc.keys(),list(map(lambda x: x.split('-') if isinstance(x,str) else x,dicc.values())))\n",
    "    \n",
    "    return [[day_dicc[key],\n",
    "            int(value[0].split(':')[0])+int(value[0].split(':')[1]),\n",
    "            int(value[1].split(':')[0])+int(value[1].split(':')[1])\n",
    "            ] if value is not None else [day_dicc[key],0,0] for key,value in check]\n",
    "\n",
    "def row_hours_to_series(series):\n",
    "    \"\"\"\n",
    "    This function takes a column from a koalas dataframe that contains a dictionary with each day of the week as a key and\n",
    "    the opening and closing schedules for the day as the value.\n",
    "    The function returns a koalas series whose elements are lists of lists in the same format as the outputed by the \n",
    "    'row_hours_to_list' function.\n",
    "\n",
    "    Parameters:\n",
    "    - series: koalas series\n",
    "    \"\"\"\n",
    "    series_mode = row_hours_to_list(series.mode().iloc[0])\n",
    "    series_output = []\n",
    "    for index, value in series.items():\n",
    "        if value is None:\n",
    "            series_output.append(series_mode)\n",
    "        else:\n",
    "            series_output.append(row_hours_to_list(value))\n",
    "    return ks.Series(series_output)\n",
    "\n",
    "\n",
    "def get_date_as_list(value):\n",
    "    ls = value.split(', ')\n",
    "    return ls\n",
    "\n",
    "def get_total_checkins(value):\n",
    "    ls = value.split(', ')\n",
    "    return len(ls)\n",
    "\n",
    "def get_state_city(df):\n",
    "    print('SETTING OPTION')\n",
    "    ks.set_option('compute.ops_on_diff_frames', True)\n",
    "    print('GeTTING CITY LIST')\n",
    "    cities = list(df.city.to_numpy())\n",
    "    print('GeTTING STATE LIST')\n",
    "    states = list(df.state.to_numpy())\n",
    "    print('OBTAINING SERIES')\n",
    "    state_city = ks.Series([[states[i],cities[i]] for i in range(len(cities))])\n",
    "    print('CREATING COLUMN')\n",
    "    df['state_city'] = state_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BusinessEDA():\n",
    "    print('IMPORTING BUSINESS')\n",
    "    df = import_json(file = 'business.json', path = './data/')\n",
    "    print('DROPPING DUPLICATES')\n",
    "    df = drop_duplicates(df)\n",
    "    \n",
    "    ######## OPEN HOURS ##########\n",
    "    #df['hours'] = df['hours'].apply(row_hours_to_series)\n",
    "    \n",
    "    print('CHECKING STRINGS')\n",
    "    ######## CATEGORIES ##########\n",
    "    df['categories'] = df['categories'].apply(check_str_list)\n",
    "\n",
    "    ######## CITY/STATE ##########\n",
    "    print('GETTING STATE_CITY COL')\n",
    "    get_state_city(df)\n",
    "    print('DROPPING CITY & STATE COLS')\n",
    "    df = df.drop(['city', 'state'], axis=1)\n",
    "\n",
    "    print('TRYING TO UPLOAD')\n",
    "\n",
    "    try:\n",
    "        upload_to_cassandra(df, 'business')\n",
    "        print('Business uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading BUSINESS to Cassandra')\n",
    "\n",
    "BusinessEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CHECKIN')\n",
    "\n",
    "def CheckinEDA():\n",
    "    print('IMPORTING')\n",
    "    df = import_json(file = 'checkin.json', path = './data/')\n",
    "\n",
    "    print('DROPPING DUPS')\n",
    "    df = drop_duplicates(df)\n",
    "    \n",
    "    print('GETTING DATE LIST')\n",
    "    df['date'] = df['date'].apply(get_date_as_list)\n",
    "\n",
    "    #print('GETTING TOTAL')\n",
    "    #df['total'] = df['date'].apply(get_total_checkins)\n",
    "\n",
    "    try:\n",
    "        upload_to_cassandra(df, 'checkin')\n",
    "        print('Checkin uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading CHECKIN to Cassandra')\n",
    "\n",
    "CheckinEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TipsEDA():\n",
    "\n",
    "    df = import_json(file = 'tip.json', path = './data/')\n",
    "\n",
    "\n",
    "    df = drop_duplicates(df)\n",
    "\n",
    "    df = drop_bad_str(df, 'text')\n",
    "\n",
    "    df['date'] = transform_dates(df, 'date', '%Y-%m-%d')\n",
    "\n",
    "    try:\n",
    "        upload_to_cassandra(df, 'tips')\n",
    "        print('Tips uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading TIPS to Cassandra')\n",
    "\n",
    "TipsEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of user.json is (1987897, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR uploading USERS to Cassandra\n"
     ]
    }
   ],
   "source": [
    "def UserEDA():\n",
    "    df = import_json(file = 'user.json', path = './data/')\n",
    "    df = drop_duplicates(df)\n",
    "\n",
    "    df['friends'] = df['friends'].apply(check_str_list)\n",
    "\n",
    "    df['elite'] = df['elite'].apply(check_str_list)\n",
    "\n",
    "    df['yelping_since'] = transform_dates(df, 'yelping_since', '%Y-%m-%d')\n",
    "\n",
    "    try:\n",
    "        upload_to_cassandra(df, 'users')\n",
    "        print('Users uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading USERS to Cassandra')\n",
    "\n",
    "UserEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of review.json is (6990280, 9)\n",
      "DELETING DUPLICATES\n",
      "IMPUTING NEGATIVE VOTES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMING DATES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR uploading REVIEWS to Cassandra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def ReviewEDA():\n",
    "    df = import_json(file = 'review.json', path = './data/')\n",
    "    \n",
    "    print('DELETING DUPLICATES')\n",
    "    df = drop_duplicates(df)\n",
    "\n",
    "    #print('DELETING BAD ID')\n",
    "    #df['user_id'] = drop_bad_ids(df, 'user_id')\n",
    "    #df['business_id'] = drop_bad_ids(df, 'business_id')\n",
    "    #df['review_id'] = drop_bad_ids(df, 'review_id')\n",
    "\n",
    "    #print('IMPUTING NEGATIVE VOTES')\n",
    "    #impute_num(df, ['useful', 'funny', 'cool'], True) ##### REALLY SLOW\n",
    "\n",
    "    print('TRANSFORMING DATES')\n",
    "    df['date'] = transform_dates(df, 'date', '%Y-%m-%d')\n",
    "\n",
    "    try:\n",
    "        upload_to_cassandra(df, 'reviews')\n",
    "        print('Reviews uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading REVIEWS to Cassandra')\n",
    "\n",
    "ReviewEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "######## LIBRARIES IMPORT ##########\n",
    "####################################\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "import pyspark.pandas as ps\n",
    "import databricks.koalas as ks\n",
    "\n",
    "from transform_funcs import *\n",
    "\n",
    "####################################\n",
    "######## SPARK RUNNING ##########\n",
    "####################################\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.3-bin-hadoop2.7\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('SparkCassandraApp') \\\n",
    "    .config('spark.cassandra.connection.host', 'localhost') \\\n",
    "    .config('spark.cassandra.connection.port', '9042') \\\n",
    "    .config('spark.cassandra.output.consistency.level','ONE') \\\n",
    "    .master('local[2]') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "####################################\n",
    "######## PATH SETTINGS ##########\n",
    "####################################\n",
    "path_1 = \"./data/\"\n",
    "\n",
    "\n",
    "####################################\n",
    "######## COMMON FUNCTIONS ##########\n",
    "####################################\n",
    "def ImporterJSON(file:str, path:Path = path_1, format:str = 'json'):\n",
    "    '''\n",
    "    This function imports files with spark and transforms them into DataFrame using the koala library\n",
    "\n",
    "    Arguments:\n",
    "    :: file: str of the file name\n",
    "    :: path: 'path' path where the file is stored\n",
    "    :: format: 'str' file format \n",
    "\n",
    "    Returns: \n",
    "    ---------\n",
    "    Dataframe and print shape \n",
    "    '''\n",
    "    path_final = path + file\n",
    "    df = ps.read_json(path_final, lines=True)\n",
    "    print(f\"Shape of {file} is {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def UploadToCassandra(df, table_name):\n",
    "    df.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=table_name, keyspace=\"yelp\")\\\n",
    "    .mode('append')\\\n",
    "    .save()\n",
    "\n",
    "def GetTime(datetime):\n",
    "    return datetime.dt.hour()\n",
    "\n",
    "def GetWordCount(str):\n",
    "    ls = str.split()\n",
    "    return len(ls)\n",
    "\n",
    "def CountItemsFromList(value):\n",
    "    ls = value.split(', ')\n",
    "    return len(ls)\n",
    "\n",
    "def Dicc(row):\n",
    "    result = ast.literal_eval(row)\n",
    "    return result\n",
    "\n",
    "def GetAVG(dates_list:list):\n",
    "    hours_sum = 0\n",
    "    ls = dates_list.split(', ')\n",
    "    list_len = len(ls)\n",
    "    for date in ls:\n",
    "        date_ok = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "        hours_sum += date_ok.hour\n",
    "    avg_checkins = hours_sum/list_len\n",
    "    return round(avg_checkins)\n",
    "\n",
    "def GetEarliestYear(dates_list:list):\n",
    "    ls = dates_list.split(', ')\n",
    "    earliest_year = 0\n",
    "    for date in ls:\n",
    "        date_ok = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "        if earliest_year < date_ok.year:\n",
    "            earliest_year = date_ok.year\n",
    "    return earliest_year\n",
    "\n",
    "def CountByYear(dates_list, year):\n",
    "    ls = dates_list.split(', ')\n",
    "    yearly_checkins = []\n",
    "    for date in ls:\n",
    "        date_ok = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "        if date_ok.year == year:\n",
    "            yearly_checkins.append(date_ok)\n",
    "    return len(yearly_checkins)\n",
    "\n",
    "\n",
    "####################################\n",
    "    ######## REVIEWS  ##########\n",
    "####################################\n",
    "\n",
    "def ReviewEDA():\n",
    "    df = ImporterJSON(file = 'reviews.json')\n",
    "    \n",
    "    df = drop_duplicates(df)\n",
    "\n",
    "    df['user_id'] = drop_bad_ids(df, 'user_id')\n",
    "    df['business_id'] = drop_bad_ids(df, 'business_id')\n",
    "    df['review_id'] = drop_bad_ids(df, 'review_id')\n",
    "\n",
    "    impute_num(df, ['useful', 'funny', 'cool'], True)\n",
    "\n",
    "    df['hour'] = transform_dates(df, 'date', '%H')\n",
    "    df['year'] = transform_dates(df, 'date', '%Y')\n",
    "\n",
    "    # df['datetime'] = df.date.astype(datetime)\n",
    "    # df['date'] = df.datetime.apply(lambda x: x.date())\n",
    "    # df['hour'] = df.datetime.dt.hour\n",
    "    # df['year'] = df.datetime.dt.year\n",
    "\n",
    "    df['word_count'] = df.text.apply(GetWordCount)\n",
    "\n",
    "    try:\n",
    "        UploadToCassandra(df, 'reviews')\n",
    "        print('Reviews uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading REVIEWS to Cassandra')\n",
    "\n",
    "    \n",
    "\n",
    "####################################\n",
    "    ######## USERS  ##########\n",
    "####################################\n",
    "\n",
    "def UserEDA():\n",
    "    df = ImporterJSON(file = 'users.json')\n",
    "    df = drop_duplicates(df)\n",
    "    df['friends_number'] = df['friends'].apply(CountItemsFromList)\n",
    "\n",
    "    df['n_interactions_send'] = df['useful'] + df['funny'] + df['cool']\n",
    "\n",
    "    df['n_interactions_received'] = df[[ 'compliment_hot',\n",
    "    'compliment_more', 'compliment_profile', 'compliment_cute',\n",
    "    'compliment_list', 'compliment_note', 'compliment_plain',\n",
    "    'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
    "    'compliment_photos']].sum(axis=1)\n",
    "\n",
    "    df['n_years_elite'] = df['elite'].apply(CountItemsFromList)\n",
    "    df['n_years_elite'] = df['n_years_elite'].fillna(0)\n",
    "\n",
    "    try:\n",
    "        UploadToCassandra(df, 'users')\n",
    "        print('Users uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading USERS to Cassandra')\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "    ######## BUSINESS  ##########\n",
    "####################################\n",
    "\n",
    "def BusinessEDA():\n",
    "    df = ImporterJSON(file = 'business.json')\n",
    "    df = drop_duplicates(df)\n",
    "\n",
    "    ######## ATRIBUTES ##########\n",
    "    # attributes = pd.json_normalize(df['attributes'])\n",
    "    # attributes['business_id'] = df.index\n",
    "    # attributes.loc[attributes.BusinessParking == 'None']=\"{'garage': False, 'street': False, 'validated': False, 'lot': False, 'valet': False}\"\n",
    "    # attributes.BusinessParking = attributes.BusinessParking.fillna(\"{'garage': False, 'street': False, 'validated': False, 'lot': False, 'valet': False}\") # a los valore nulos le pogo False\n",
    "    # attributes.loc[attributes.Ambience == 'None']=\"{'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': False}\"\n",
    "    # attributes.Ambience = attributes.Ambience.fillna(\"{'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': False}\")\n",
    "    # parking = pd.json_normalize(attributes.BusinessParking.apply(Dicc))\n",
    "    # ambience = pd.json_normalize(attributes.Ambience.apply(Dicc))\n",
    "    # parking.set_index(df.index, inplace=True)\n",
    "    # ambience.set_index(df.index, inplace=True)\n",
    "    # attributes = attributes.drop(['BusinessParking', 'Ambience'], axis=1)\n",
    "    # attributes = pd.concat([attributes, parking, ambience], axis=1)\n",
    "    \n",
    "    ######## OPEN HOURS ##########\n",
    "    #openhours = pd.json_normalize(df['hours'])\n",
    "    #openhours['business_id'] = df.index\n",
    "    \n",
    "    ######## CATEGORIES ##########\n",
    "    df['categories'] = df['categories'].apply(check_str_list)\n",
    "    #df['city_state'] = list(df['state'], df['city'])\n",
    "\n",
    "    \n",
    "    # categories = pd.json_normalize(df['categories'])\n",
    "    # categories = df['categories'].str.split(', ', expand=True)\n",
    "    # categories = categories.T.stack().groupby('business_id').apply(list).reset_index(name='categories')\n",
    "    # mlb = MultiLabelBinarizer()\n",
    "    # cat_full = categories.join(pd.DataFrame(mlb.fit_transform(categories.pop('categories')),\n",
    "    #                         columns=mlb.classes_,\n",
    "    #                         index=categories.index))\n",
    "\n",
    "    ######## COPYING TO DATALAKE ##########\n",
    "    attributes.to_csv('./data/attributes.csv', index=False)\n",
    "    openhours.to_csv('./data/openhours.csv', index=False)\n",
    "    cat_full.to_csv('./data/categories.csv', index=False)\n",
    "\n",
    "    df.drop(['attributes', 'hours', 'categories'], axis=1, inplace=True)\n",
    "\n",
    "    try:\n",
    "        UploadToCassandra(df, 'business')\n",
    "        print('Business uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading BUSINESS to Cassandra')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "    ######## CHECKIN  ##########\n",
    "####################################\n",
    "\n",
    "def CheckinEDA():\n",
    "    df = ImporterJSON(file = 'checkin.json')\n",
    "\n",
    "    df = drop_duplicates(df)\n",
    "    \n",
    "    df['number_visits'] = df['date'].apply(CountItemsFromList)\n",
    "\n",
    "    df['avg_hour'] = df['date'].apply(GetAVG)\n",
    "\n",
    "    df['earliest_year'] = df['date'].apply(GetEarliestYear)\n",
    "\n",
    "    for x in range(2010, 2022):\n",
    "        df[str(x)] = df.date.apply(CountByYear, args=(x,))\n",
    "\n",
    "    try:\n",
    "        UploadToCassandra(df, 'checkin')\n",
    "        print('Checkin uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading CHECKIN to Cassandra')\n",
    "\n",
    "\n",
    "####################################\n",
    "    ######## TIPS  ##########\n",
    "####################################\n",
    "\n",
    "def TipsEDA():\n",
    "    df = ImporterJSON(file = 'tips.json')\n",
    "    df = drop_duplicates(df)\n",
    "\n",
    "    df = drop_bad_str(df, 'text')\n",
    "\n",
    "    df['date'] = ps.to_datetime(df['date'])\n",
    "    df['year'] = df.datetime.dt.year\n",
    "    df['word_count'] = df.text.apply(GetWordCount)\n",
    "\n",
    "    try:\n",
    "        UploadToCassandra(df, 'tips')\n",
    "        print('Tips uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading TIPS to Cassandra')\n",
    "\n",
    "\n",
    "####################################\n",
    "######## SENTIMENT UPLOAD  #######\n",
    "####################################\n",
    "\n",
    "def SentimentUpload():\n",
    "    return \"\"\n",
    "\n",
    "####################################\n",
    "######### QUERY FUNCTIONS  ##########\n",
    "####################################\n",
    "\n",
    "def MakeQuery(query):\n",
    "    sqlContext = SQLContext(spark)\n",
    "    ds = sqlContext \\\n",
    "    .read \\\n",
    "    .format('org.apache.spark.sql.cassandra') \\\n",
    "    .options(table='business', keyspace='yelp') \\\n",
    "    .load()\n",
    "\n",
    "    try:\n",
    "        ds.show(10) \n",
    "        print('Query executed')\n",
    "        return \"OK\"\n",
    "    except:\n",
    "        print('ERROR executing query')\n",
    "        return \"ERROR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "######## LIBRARIES IMPORT ##########\n",
    "####################################\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datetime import datetime\n",
    "import databricks.koalas as ks\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.3-bin-hadoop2.7\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "\n",
    "####################################\n",
    "######## SPARK RUNNING ##########\n",
    "####################################\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "####################################\n",
    "######## PATH SETTINGS ##########\n",
    "####################################\n",
    "path_1 = \"./data/\"\n",
    "\n",
    "\n",
    "####################################\n",
    "######## COMMON FUNCTIONS ##########\n",
    "####################################\n",
    "def ImporterJSON(file:str, path:Path = path_1, format:str = 'json'):\n",
    "    '''\n",
    "    This function imports files with spark and transforms them into DataFrame using the koala library\n",
    "\n",
    "    Arguments:\n",
    "    :: path: 'path' path where the file is stored\n",
    "    :: format: 'str' file format \n",
    "\n",
    "    Returns: \n",
    "    ---------\n",
    "    Dataframe and print shape \n",
    "    '''\n",
    "    path_final = path + file\n",
    "    df = spark.read.load(path, format=format)\n",
    "    df = df.to_koalas()\n",
    "    print(df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "def GetTime(datetime):\n",
    "    return datetime.dt.hour()\n",
    "\n",
    "def GetWordCount(str):\n",
    "    ls = str.split()\n",
    "    return len(ls)\n",
    "\n",
    "def CountItemsFromList(value):\n",
    "    ls = value.split(', ')\n",
    "    return len(ls)\n",
    "\n",
    "def Dicc(row):\n",
    "    result = ast.literal_eval(row)\n",
    "    return result\n",
    "\n",
    "def GetAVG(dates_list:list):\n",
    "    hours_sum = 0\n",
    "    ls = dates_list.split(', ')\n",
    "    list_len = len(ls)\n",
    "    for date in ls:\n",
    "        date_ok = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "        hours_sum += date_ok.hour\n",
    "    avg_checkins = hours_sum/list_len\n",
    "    return round(avg_checkins)\n",
    "\n",
    "def GetEarliestYear(dates_list:list):\n",
    "    ls = dates_list.split(', ')\n",
    "    earliest_year = 0\n",
    "    for date in ls:\n",
    "        date_ok = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "        if earliest_year < date_ok.year:\n",
    "            earliest_year = date_ok.year\n",
    "    return earliest_year\n",
    "\n",
    "def CountByYear(dates_list, year):\n",
    "    ls = dates_list.split(', ')\n",
    "    yearly_checkins = []\n",
    "    for date in ls:\n",
    "        date_ok = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "        if date_ok.year == year:\n",
    "            yearly_checkins.append(date_ok)\n",
    "    return len(yearly_checkins)\n",
    "\n",
    "####################################\n",
    "    ######## REVIEWS  ##########\n",
    "####################################\n",
    "\n",
    "def ReviewEDA():\n",
    "    df = ImporterJSON(file = 'reviews.json')\n",
    "\n",
    "    df['datetime'] = df.date.astype(datetime)\n",
    "    df['date'] = df.datetime.apply(lambda x: x.date())\n",
    "    df['hour'] = df.datetime.dt.hour\n",
    "    df['year'] = df.datetime.dt.year\n",
    "    df['word_count'] = df.text.apply(GetWordCount)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "    ######## USERS  ##########\n",
    "####################################\n",
    "\n",
    "def UserEDA():\n",
    "    df = ImporterJSON(file = 'users.json')\n",
    "\n",
    "    df['friends_number'] = df['friends'].apply(CountItemsFromList)\n",
    "\n",
    "    df['n_interactions_send'] = df['useful'] + df['funny'] + df['cool']\n",
    "\n",
    "    df['n_interactions_received'] = df[[ 'compliment_hot',\n",
    "    'compliment_more', 'compliment_profile', 'compliment_cute',\n",
    "    'compliment_list', 'compliment_note', 'compliment_plain',\n",
    "    'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
    "    'compliment_photos']].sum(axis=1)\n",
    "\n",
    "    df['n_years_elite'] = df['elite'].apply(CountItemsFromList)\n",
    "    df['n_years_elite'] = df['n_years_elite'].fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "    ######## BUSINESS  ##########\n",
    "####################################\n",
    "\n",
    "def BusinessEDA():\n",
    "    df = ImporterJSON(file = 'business.json')\n",
    "\n",
    "\n",
    "    ######## ATRIBUTES ##########\n",
    "    attributes = pd.json_normalize(df['attributes'])\n",
    "    attributes['business_id'] = df.index\n",
    "    attributes.loc[attributes.BusinessParking == 'None']=\"{'garage': False, 'street': False, 'validated': False, 'lot': False, 'valet': False}\"\n",
    "    attributes.BusinessParking = attributes.BusinessParking.fillna(\"{'garage': False, 'street': False, 'validated': False, 'lot': False, 'valet': False}\") # a los valore nulos le pogo False\n",
    "    attributes.loc[attributes.Ambience == 'None']=\"{'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': False}\"\n",
    "    attributes.Ambience = attributes.Ambience.fillna(\"{'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': False}\")\n",
    "    parking = pd.json_normalize(attributes.BusinessParking.apply(Dicc))\n",
    "    ambience = pd.json_normalize(attributes.Ambience.apply(Dicc))\n",
    "    parking.set_index(df.index, inplace=True)\n",
    "    ambience.set_index(df.index, inplace=True)\n",
    "    attributes = attributes.drop(['BusinessParking', 'Ambience'], axis=1)\n",
    "    attributes = pd.concat([attributes, parking, ambience], axis=1)\n",
    "    \n",
    "    ######## OPEN HOURS ##########\n",
    "    openhours = pd.json_normalize(df['hours'])\n",
    "    openhours['business_id'] = df.index\n",
    "    \n",
    "    ######## CATEGORIES ##########\n",
    "    categories = pd.json_normalize(df['categories'])\n",
    "    categories = df['categories'].str.split(', ', expand=True)\n",
    "    categories = categories.T.stack().groupby('business_id').apply(list).reset_index(name='categories')\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    cat_full = categories.join(pd.DataFrame(mlb.fit_transform(categories.pop('categories')),\n",
    "                            columns=mlb.classes_,\n",
    "                            index=categories.index))\n",
    "\n",
    "    ######## COPYING TO DATALAKE ##########\n",
    "    attributes.to_csv('./data/attributes.csv', index=False)\n",
    "    openhours.to_csv('./data/openhours.csv', index=False)\n",
    "    cat_full.to_csv('./data/categories.csv', index=False)\n",
    "\n",
    "    df.drop(['attributes', 'hours', 'categories'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "    ######## CHECKIN  ##########\n",
    "####################################\n",
    "\n",
    "def CheckinEDA():\n",
    "    df = ImporterJSON(file = 'checkin.json')\n",
    "    \n",
    "    df['number_visits'] = df['date'].apply(CountItemsFromList)\n",
    "\n",
    "    df['avg_hour'] = df['date'].apply(GetAVG)\n",
    "\n",
    "    df['earliest_year'] = df['date'].apply(GetEarliestYear)\n",
    "\n",
    "    for x in range(2010, 2022):\n",
    "        df[str(x)] = df.date.apply(CountByYear, args=(x,))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "    ######## TIPS  ##########\n",
    "####################################\n",
    "\n",
    "def TipsEDA():\n",
    "    df = ImporterJSON(file = 'tips.json')\n",
    "\n",
    "    df['date'] = ps.to_datetime(df['date'])\n",
    "    df['year'] = df.datetime.dt.year\n",
    "    df['word_count'] = df.text.apply(GetWordCount)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def trim_all_columns(df):\n",
    "#     \"\"\"\n",
    "#     > If the value is a string, strip whitespace from the ends of the string. Otherwise, return the value\n",
    "    \n",
    "#     Arguments: \n",
    "#     --------------------------------   \n",
    "#     :param df: The dataframe you want to trim\n",
    "\n",
    "#     Return\n",
    "#     --------------------------------\n",
    "#     A dataframe with all the values trimmed.\n",
    "#     \"\"\"\n",
    "#     trim_strings = lambda x: x.strip() if isinstance(x, str) else x\n",
    "#     return df.applymap(trim_strings)\n",
    "\n",
    "# def normalize_column(df, column_name):\n",
    "#     \"\"\"\n",
    "#     > This function takes a dataframe and a column name as input, and returns a new dataframe with the\n",
    "#     column normalized\n",
    "\n",
    "#     Arguments: \n",
    "#     --------------------------------\n",
    "#     :param df: the dataframe\n",
    "#     :param column_name: The name of the column you want to normalize\n",
    "#     \"\"\"\n",
    "#     df[column_name] = df[column_name].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "#     return df[column_name]\n",
    "\n",
    "# # Source: https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string\n",
    "# def clean_values(series, to_replace, value = '', regex = True):\n",
    "#     \"\"\"\n",
    "#     > This function takes a pandas series, a list of values to replace, and a value to replace them with,\n",
    "#     and returns a series with the values replaced.\n",
    "\n",
    "#         Arguments: \n",
    "#     --------------------------------\n",
    "#     :param series: the series you want to clean\n",
    "#     :param to_replace: The value or list of values to be replaced\n",
    "#     :param value: the value to replace the to_replace values with\n",
    "#     :param regex: If True, assumes the to_replace pattern is a regular expression, defaults to True\n",
    "#     (optional)\n",
    "#     \"\"\"\n",
    "#     for i in to_replace:\n",
    "#         series = series.str.replace(i, value, regex=regex)\n",
    "#     return series\n",
    "\n",
    "# def get_lat_lon(address, access_key = '2e843c7ee44a8f52742a8168d0121a0a', URL = \"http://api.positionstack.com/v1/forward\"):\n",
    "#     \"\"\"\n",
    "#     > It takes an address and returns the latitude and longitude of that address\n",
    "\n",
    "#      Arguments: \n",
    "#     --------------------------------   \n",
    "#     :param address: The address you want to get the latitude and longitude for\n",
    "#     :param access_key: This is the access key that you get from the website, defaults to\n",
    "#     2e843c7ee44a8f52742a8168d0121a0a (optional)\n",
    "#     :param URL: The URL of the API endpoint, defaults to http://api.positionstack.com/v1/forward\n",
    "#     (optional)\n",
    "\n",
    "#     Return\n",
    "#     --------------------------------\n",
    "#     A tuple of latitude and longitude\n",
    "#     \"\"\"\n",
    "#     PARAMS = {'access_key': access_key, 'query': address}\n",
    "#     r = requests.get(url = URL, params = PARAMS)\n",
    "#     data = r.json()\n",
    "#     return data['data'][0]['latitude'], data['data'][0]['longitude']\n",
    "\n",
    "# def run_exps(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     > This function takes in training and test data, and then runs a bunch of models on it, returning a\n",
    "#     dataframe of the results\n",
    "    \n",
    "#     Arguments: \n",
    "#     --------------------------------\n",
    "#     :param X_train: training split \n",
    "#     :param y_train: training target vector\n",
    "#     :param X_test: test split\n",
    "#     :param y_test: test target vector\n",
    "\n",
    "#     Types: \n",
    "#     --------------------------------\n",
    "#     :type X_train: pd.DataFrame\n",
    "#     :type y_train: pd.DataFrame\n",
    "#     :type X_test: pd.DataFrame\n",
    "#     :type y_test: pd.DataFrame\n",
    "\n",
    "#     Return\n",
    "#     --------------------------------\n",
    "#     A dataframe of predictions\n",
    "#     \"\"\"\n",
    "    \n",
    "#     dfs = []\n",
    "\n",
    "#     dt = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "#     models = [\n",
    "#         ('LogReg', LogisticRegression()), \n",
    "#         ('RF', RandomForestClassifier()),\n",
    "#         ('KNN', KNeighborsClassifier()),\n",
    "#         ('GNB', GaussianNB()),\n",
    "#         ('XGB', XGBClassifier()),\n",
    "#         ('ADA', AdaBoostClassifier(base_estimator=dt))\n",
    "#         ]\n",
    "#     results = []\n",
    "#     names = []\n",
    "\n",
    "#     scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n",
    "\n",
    "#     target_names = ['malignant', 'benign']\n",
    "\n",
    "#     for name, model in models:\n",
    "#             kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "#             cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "#             clf = model.fit(X_train, y_train)\n",
    "#             y_pred = clf.predict(X_test)\n",
    "#             print(name)\n",
    "#             print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "            \n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     this_df = pd.DataFrame(cv_results)\n",
    "#     this_df['model'] = name\n",
    "#     dfs.append(this_df)\n",
    "#     final = pd.concat(dfs, ignore_index=True)\n",
    "#     return final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
