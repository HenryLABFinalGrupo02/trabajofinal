{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/23 19:21:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/11/23 19:21:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "######## LIBRARIES IMPORT ##########\n",
    "####################################\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import databricks.koalas as ks\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "####################################\n",
    "######## SPARK RUNNING ##########\n",
    "####################################\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/23 19:21:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "######## LIBRARIES IMPORT ##########\n",
    "####################################\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import databricks.koalas as ks\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "####################################\n",
    "######## SPARK RUNNING ##########\n",
    "####################################\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "'''\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('SparkCassandraApp') \\\n",
    "    .config('spark.cassandra.connection.host', 'localhost') \\\n",
    "    .config('spark.cassandra.connection.port', '9042') \\\n",
    "    .config('spark.cassandra.output.consistency.level','ONE') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "'''\n",
    "ks.set_option('compute.ops_on_diff_frames', True)\n",
    "\n",
    "####################################\n",
    "######## PATH SETTINGS ##########\n",
    "####################################\n",
    "path_1 = \"./\"\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "\n",
    "def values_type(dataframe, column): \n",
    "    \"\"\"\n",
    "    This functions returns a set that contains the data types contained within a column of a pandas or koalas dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: pandas or koalas dataframe\n",
    "    - column: the name of the column to analize\n",
    "    \"\"\"\n",
    "    types = set()\n",
    "    for value in dataframe[column].to_numpy():\n",
    "        types.add(type(value))\n",
    "    return types\n",
    "\n",
    "# DEALING WITH DUPLICATED REGISTERS\n",
    "\n",
    "def drop_duplicates(Table):\n",
    "    \"\"\"\n",
    "    Returns a Dataframe with no duplicates\n",
    "\n",
    "    Parameters:\n",
    "    - Table: Pandas or Koalas dataframe\n",
    "    \"\"\"\n",
    "    return Table.drop_duplicates()\n",
    "\n",
    "\n",
    "####################################\n",
    "######## COMMON FUNCTIONS ##########\n",
    "####################################\n",
    "def import_json(file:str, path:Path = path_1, format:str = 'json'):\n",
    "    '''\n",
    "    This function imports files with spark and transforms them into DataFrame using the koala library\n",
    "\n",
    "    Arguments:\n",
    "    :: file: str of the file name\n",
    "    :: path: 'path' path where the file is stored\n",
    "    :: format: 'str' file format \n",
    "\n",
    "    Returns: \n",
    "    ---------\n",
    "    Dataframe and print shape \n",
    "    '''\n",
    "    path_final = path + file\n",
    "    print('READING JSON')\n",
    "    df = ks.read_json(path_final, lines=True)\n",
    "    print(f\"Shape of {file} is {df.shape}\")\n",
    "    return df\n",
    "    \n",
    "\n",
    "def upload_to_cassandra(df, table_name):\n",
    "    df.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=table_name, keyspace=\"yelp\")\\\n",
    "    .mode('append')\\\n",
    "    .save()\n",
    "\n",
    "\n",
    "# ID VALIDATION\n",
    "\n",
    "def check_id_chars(Table, id_column):\n",
    "    \"\"\"\n",
    "    Checks if the strings in an ID column have the required characters (20).\n",
    "    This function is meant to be called within the 'drop_bad_ids' function.\n",
    "    Returns a list of indexes at which the column has an invalid ID.\n",
    "\n",
    "    Parameters:\n",
    "    - Table: Pandas or Koalas dataframe\n",
    "    - id_column: column containing 22 character ID's\n",
    "    \"\"\"\n",
    "    problems = []\n",
    "    for index, value in Table[id_column].items() :\n",
    "        if len(value) != 22:\n",
    "            problems.append(index)\n",
    "    return problems\n",
    "\n",
    "def drop_bad_ids(Table, id_column):\n",
    "    \"\"\"\n",
    "    This function removes the rows in a table where an ID is not valid.\n",
    "    Returns a table with only valid ID's in the passed column.\n",
    "\n",
    "    Parameters:\n",
    "    - Table: Koalas dataframe\n",
    "    - id_column: column containing 22 character ID's\n",
    "    \"\"\"\n",
    "    id_list = check_id_chars(Table, id_column)\n",
    "    return Table[ks.Series((~Table.index.isin(id_list)).to_list())].reset_index(drop=True)\n",
    "\n",
    "# NUMERIC VALUES\n",
    "\n",
    "def impute_num(Table, col_list, absolute=False):\n",
    "    \"\"\"\n",
    "    This function replaces missing values in numeric columns with 0.\n",
    "    If the 'absolute' parameter is passed, the function also converts \n",
    "    the numeric columns into their absolute value.\n",
    "\n",
    "    Parameters:\n",
    "    - Table: Pandas or Koalas dataframe\n",
    "    - col_list: list of numeric columns with missing values to be imputed\n",
    "    - absolute: boolean, decides if the column will contain absolute values. Default: False.\n",
    "    \"\"\"\n",
    "    for col in col_list:\n",
    "        Table[col].fillna(0)\n",
    "        if absolute:\n",
    "            Table[col] = Table[col].apply(lambda x: abs(x))\n",
    "\n",
    "# STRING VALUES\n",
    "\n",
    "def clean_string(string):\n",
    "    \"\"\"\n",
    "    This function cleans strings by removing whitespaces at the beginning and \n",
    "    at the end of the string, replacing double spaces with single spaces and\n",
    "    converting the string to lower case.\n",
    "    It is meant to be used within the 'drop_bad_str' function.\n",
    "    Returns a clean string.\n",
    "\n",
    "    Parameters:\n",
    "    - string: some string to be cleaned\n",
    "    \"\"\"\n",
    "    new_str = string.strip().replace('  ',' ').lower()\n",
    "    return new_str\n",
    "\n",
    "def drop_bad_str(Table, col):\n",
    "    \"\"\"\n",
    "    This function takes a Dataframe and the name of a column that contains string values, \n",
    "    imputes missing values in the column, cleans it's strings and removes registers where \n",
    "    the string in the column has 2 or less characters.\n",
    "    The function returns the dataframe after performing the above mentioned transformations\n",
    "    and dropping the unwanted registers.\n",
    "\n",
    "    Parameters:\n",
    "    - Table: Pandas or Koalas dataframe\n",
    "    - col: string, the name of the column to transform\n",
    "    \"\"\"\n",
    "    T_ok = Table.copy()\n",
    "    T_ok[col] = T_ok[col].fillna('NO DATA')\n",
    "    T_ok[col] = T_ok[col].apply(clean_string)\n",
    "    bad_strs = []\n",
    "    for index, tip in T_ok[col].items():\n",
    "        if len(tip) <=2:\n",
    "            bad_strs.append(index)\n",
    "    return T_ok[ks.Series((~Table.index.isin(bad_strs)).to_list())].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# DATETIME VALUES\n",
    "\n",
    "def transform_dates(dataframe,column,format):\n",
    "    \"\"\"\n",
    "    This function recieves 1) a dataframe, 2) the name of a column containing timestamp values\n",
    "    and 3) a date format. It returns the dataframe after transforming the column to the desired \n",
    "    format.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: a Koalas dataframe\n",
    "    - column: the name of the column containing timestamp values\n",
    "    - format: the datetime format to which the column will be transformed\n",
    "    \"\"\"\n",
    "    series = ks.to_datetime(dataframe[column], errors='coerce')\n",
    "    mode = series.mode().iloc[0].strftime(format)\n",
    "    series = series.apply(lambda x: mode if (x is pd.NaT) else x.strftime(format))\n",
    "    return series\n",
    "\n",
    "# LISTS OF STRINGS\n",
    "\n",
    "def check_str_list(ls):\n",
    "    \"\"\"\n",
    "    This function recieves a list and returns a second list containing only the strings from the\n",
    "    original list. In case there were none, it returns an empty list. If a None value is passed, \n",
    "    the function returns an empty list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ls_ok = []\n",
    "        for x in ls:\n",
    "            if type(x) == str:\n",
    "                ls_ok.append(x)\n",
    "        return ls_ok\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# DICTIONARY\n",
    "\n",
    "def row_hours_to_list(row):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, each sublist containing the day of the week, it's opening hour and it's closing hour. E.g.: [[1,8,18],[2,8,18]...]\n",
    "\n",
    "    Parameters:\n",
    "    - row: pyspark row object\n",
    "    \"\"\"\n",
    "    dicc = row.asDict()\n",
    "    day_dicc = {\n",
    "        'Monday': 1,\n",
    "        'Tuesday': 2,\n",
    "        'Wednesday': 3,\n",
    "        'Thursday': 4,\n",
    "        'Friday': 5,\n",
    "        'Saturday': 6,\n",
    "        'Sunday': 7\n",
    "    }\n",
    "\n",
    "    check = zip(dicc.keys(),list(map(lambda x: x.split('-') if isinstance(x,str) else x,dicc.values())))\n",
    "    \n",
    "    return [[day_dicc[key],\n",
    "            int(value[0].split(':')[0])+int(value[0].split(':')[1]),\n",
    "            int(value[1].split(':')[0])+int(value[1].split(':')[1])\n",
    "            ] if value is not None else [day_dicc[key],0,0] for key,value in check]\n",
    "\n",
    "def row_hours_to_series(series):\n",
    "    \"\"\"\n",
    "    This function takes a column from a koalas dataframe that contains a dictionary with each day of the week as a key and\n",
    "    the opening and closing schedules for the day as the value.\n",
    "    The function returns a koalas series whose elements are lists of lists in the same format as the outputed by the \n",
    "    'row_hours_to_list' function.\n",
    "\n",
    "    Parameters:\n",
    "    - series: koalas series\n",
    "    \"\"\"\n",
    "    series_mode = row_hours_to_list(series.mode().iloc[0])\n",
    "    series_output = []\n",
    "    for index, value in series.items():\n",
    "        if value is None:\n",
    "            series_output.append(series_mode)\n",
    "        else:\n",
    "            series_output.append(row_hours_to_list(value))\n",
    "    return ks.Series(series_output)\n",
    "\n",
    "\n",
    "def get_date_as_list(value):\n",
    "    ls = value.split(', ')\n",
    "    return ls\n",
    "\n",
    "def get_total_checkins(value):\n",
    "    ls = value.split(', ')\n",
    "    return len(ls)\n",
    "\n",
    "def get_state_city(df):\n",
    "    print('SETTING OPTION')\n",
    "    ks.set_option('compute.ops_on_diff_frames', True)\n",
    "    print('GeTTING CITY LIST')\n",
    "    cities = list(df.city.to_numpy())\n",
    "    print('GeTTING STATE LIST')\n",
    "    states = list(df.state.to_numpy())\n",
    "    print('OBTAINING SERIES')\n",
    "    state_city = ks.Series([[states[i],cities[i]] for i in range(len(cities))])\n",
    "    print('CREATING COLUMN')\n",
    "    df['state_city'] = state_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BusinessEDA():\n",
    "    print('IMPORTING BUSINESS')\n",
    "    df = import_json(file = 'business.json', path = './data/')\n",
    "    print('DROPPING DUPLICATES')\n",
    "    df = drop_duplicates(df)\n",
    "    \n",
    "    ######## OPEN HOURS ##########\n",
    "    #df['hours'] = df['hours'].apply(row_hours_to_series)\n",
    "    \n",
    "    print('CHECKING STRINGS')\n",
    "    ######## CATEGORIES ##########\n",
    "    df['categories'] = df['categories'].apply(check_str_list)\n",
    "\n",
    "    ######## CITY/STATE ##########\n",
    "    print('GETTING STATE_CITY COL')\n",
    "    get_state_city(df)\n",
    "    print('DROPPING CITY & STATE COLS')\n",
    "    df = df.drop(['city', 'state'], axis=1)\n",
    "\n",
    "    print('TRYING TO UPLOAD')\n",
    "\n",
    "    try:\n",
    "        upload_to_cassandra(df, 'business')\n",
    "        print('Business uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading BUSINESS to Cassandra')\n",
    "\n",
    "BusinessEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CHECKIN')\n",
    "\n",
    "def CheckinEDA():\n",
    "    print('IMPORTING')\n",
    "    df = import_json(file = 'checkin.json', path = './data/')\n",
    "\n",
    "    print('DROPPING DUPS')\n",
    "    df = drop_duplicates(df)\n",
    "    \n",
    "    print('GETTING DATE LIST')\n",
    "    df['date'] = df['date'].apply(get_date_as_list)\n",
    "\n",
    "    #print('GETTING TOTAL')\n",
    "    #df['total'] = df['date'].apply(get_total_checkins)\n",
    "\n",
    "    try:\n",
    "        upload_to_cassandra(df, 'checkin')\n",
    "        print('Checkin uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading CHECKIN to Cassandra')\n",
    "\n",
    "CheckinEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TipsEDA():\n",
    "\n",
    "    df = import_json(file = 'tip.json', path = './data/')\n",
    "\n",
    "\n",
    "    df = drop_duplicates(df)\n",
    "\n",
    "    df = drop_bad_str(df, 'text')\n",
    "\n",
    "    df['date'] = transform_dates(df, 'date', '%Y-%m-%d')\n",
    "\n",
    "    try:\n",
    "        upload_to_cassandra(df, 'tips')\n",
    "        print('Tips uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading TIPS to Cassandra')\n",
    "\n",
    "TipsEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of user.json is (1987897, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR uploading USERS to Cassandra\n"
     ]
    }
   ],
   "source": [
    "def UserEDA():\n",
    "    df = import_json(file = 'user.json', path = './data/')\n",
    "    df = drop_duplicates(df)\n",
    "\n",
    "    df['friends'] = df['friends'].apply(check_str_list)\n",
    "\n",
    "    df['elite'] = df['elite'].apply(check_str_list)\n",
    "\n",
    "    df['yelping_since'] = transform_dates(df, 'yelping_since', '%Y-%m-%d')\n",
    "\n",
    "    try:\n",
    "        upload_to_cassandra(df, 'users')\n",
    "        print('Users uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading USERS to Cassandra')\n",
    "\n",
    "UserEDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of review.json is (6990280, 9)\n",
      "DELETING DUPLICATES\n",
      "IMPUTING NEGATIVE VOTES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMING DATES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR uploading REVIEWS to Cassandra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def ReviewEDA():\n",
    "    df = import_json(file = 'review.json', path = './data/')\n",
    "    \n",
    "    print('DELETING DUPLICATES')\n",
    "    df = drop_duplicates(df)\n",
    "\n",
    "    #print('DELETING BAD ID')\n",
    "    #df['user_id'] = drop_bad_ids(df, 'user_id')\n",
    "    #df['business_id'] = drop_bad_ids(df, 'business_id')\n",
    "    #df['review_id'] = drop_bad_ids(df, 'review_id')\n",
    "\n",
    "    #print('IMPUTING NEGATIVE VOTES')\n",
    "    #impute_num(df, ['useful', 'funny', 'cool'], True) ##### REALLY SLOW\n",
    "\n",
    "    print('TRANSFORMING DATES')\n",
    "    df['date'] = transform_dates(df, 'date', '%Y-%m-%d')\n",
    "\n",
    "    try:\n",
    "        upload_to_cassandra(df, 'reviews')\n",
    "        print('Reviews uploaded to Cassandra')\n",
    "        return \"Done\"\n",
    "    except:\n",
    "        print('ERROR uploading REVIEWS to Cassandra')\n",
    "\n",
    "ReviewEDA()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
