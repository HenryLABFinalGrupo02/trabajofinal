{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing basic libraries for loading the big datasets which will be used for incremental loading: REVIEW and TIP\n",
        "\n",
        "This small samples will be uploaded in the Data Lake (S3 Bucket) and then will be uploaded to the DataWarehouse when detected by S3 Sensors on Airflows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urfjUL1BLSOU"
      },
      "outputs": [],
      "source": [
        "#### LIBRARIES\n",
        "\n",
        "#### BASIC\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os \n",
        "import json\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
        "#### SPARK\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.pandas as ps\n",
        "import databricks.koalas as ks\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "#### SETTINGS\n",
        "%matplotlib inline\n",
        "spark.sparkContext.setLogLevel(\"OFF\")\n",
        "ps.options.plotting.backend = 'matplotlib'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3sD4WaLMLtEa"
      },
      "outputs": [],
      "source": [
        "review = ks.read_json(\"./data/review.json\", lines= True)\n",
        "tip = ks.read_json(\"./data/tip.json\", lines= True)\n",
        "checkin = ks.read_json(\"./data/checkin.json\", lines= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iMmacgH0bWvO"
      },
      "outputs": [],
      "source": [
        "def transform_dates(dataframe,column,format):\n",
        "    \"\"\"\n",
        "    This function recieves 1) a dataframe, 2) the name of a column containing timestamp values\n",
        "    and 3) a date format. It returns the dataframe after transforming the column to the desired \n",
        "    format.\n",
        "    \n",
        "    Parameters:\n",
        "    - dataframe: a Koalas dataframe\n",
        "    - column: the name of the column containing timestamp values\n",
        "    - format: the datetime format to which the column will be transformed\n",
        "    \"\"\"\n",
        "    series = ks.to_datetime(dataframe[column], errors='coerce')\n",
        "    mode = series.mode().iloc[0].strftime(format)\n",
        "    series = series.apply(lambda x: mode if (x is pd.NaT) else x.strftime(format))\n",
        "    return series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj3eOVkFbf0q",
        "outputId": "454bec59-7641-4c3c-f253-2f7ae3fd1fe3"
      },
      "outputs": [],
      "source": [
        "review['date'] = transform_dates(review, 'date', '%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tip['date'] = transform_dates(tip, 'date', '%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "review_small = review[review['date']>'2021-10-01']\n",
        "review_small.to_json('./data/review_small.json')\n",
        "review_big = review[review['date']<'2021-10-01']\n",
        "review_big.to_json('./data/review_big.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "tip_small = tip[tip['date']>'2021-10-01']\n",
        "tip_small.to_json('./data/tip_small.json')\n",
        "tip_big = tip[tip['date']<'2021-10-01']\n",
        "tip_big.to_json('./data/tip_big.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making a tiny sample to try with S3 BUCKET\n",
        "\n",
        "TEST = For testing\n",
        "REST = For uploading the day of the DEMO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "review_small = pd.read_json('../AIRFLOW-SPARK/data/initial_load/reviews_tiny.json', lines=True)\n",
        "\n",
        "review_test = review_small[review_small['date']>'2022-01-01']\n",
        "review_rest = review_small[review_small['date']<'2022-01-01']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "review_test.to_json('../AIRFLOW-SPARK/data/initial_load/review_test.json')\n",
        "review_rest.to_json('../AIRFLOW-SPARK/data/initial_load/review_rest.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "tip_small = pd.read_json('../AIRFLOW-SPARK/data/initial_load/tip_tiny.json', lines=True)\n",
        "\n",
        "tip_test = tip_small[tip_small['date']>'2022-01-01']\n",
        "tip_rest = tip_small[tip_small['date']<'2022-01-01']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "tip_test.to_json('../AIRFLOW-SPARK/data/initial_load/tip_test.json')\n",
        "tip_rest.to_json('../AIRFLOW-SPARK/data/initial_load/tip_rest.json')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "3cea5bbf73bc3b494db0a8d4c59c841f216f2a2b25ad88b6d6d29f1c6d275726"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
