{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "urfjUL1BLSOU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/11/30 15:50:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "22/11/30 15:50:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        }
      ],
      "source": [
        "#### LIBRARIES\n",
        "\n",
        "#### BASIC\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os \n",
        "import json\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
        "#### SPARK\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.pandas as ps\n",
        "import databricks.koalas as ks\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "#### SETTINGS\n",
        "%matplotlib inline\n",
        "spark.sparkContext.setLogLevel(\"OFF\")\n",
        "ps.options.plotting.backend = 'matplotlib'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3sD4WaLMLtEa"
      },
      "outputs": [],
      "source": [
        "review = ks.read_json(\"./data/review.json\", lines= True)\n",
        "tip = ks.read_json(\"./data/tip.json\", lines= True)\n",
        "checkin = ks.read_json(\"./data/checkin.json\", lines= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iMmacgH0bWvO"
      },
      "outputs": [],
      "source": [
        "def transform_dates(dataframe,column,format):\n",
        "    \"\"\"\n",
        "    This function recieves 1) a dataframe, 2) the name of a column containing timestamp values\n",
        "    and 3) a date format. It returns the dataframe after transforming the column to the desired \n",
        "    format.\n",
        "    \n",
        "    Parameters:\n",
        "    - dataframe: a Koalas dataframe\n",
        "    - column: the name of the column containing timestamp values\n",
        "    - format: the datetime format to which the column will be transformed\n",
        "    \"\"\"\n",
        "    series = ks.to_datetime(dataframe[column], errors='coerce')\n",
        "    mode = series.mode().iloc[0].strftime(format)\n",
        "    series = series.apply(lambda x: mode if (x is pd.NaT) else x.strftime(format))\n",
        "    return series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj3eOVkFbf0q",
        "outputId": "454bec59-7641-4c3c-f253-2f7ae3fd1fe3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/spark/python/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "/opt/spark/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/opt/spark/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
            "/opt/spark/python/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "review['date'] = transform_dates(review, 'date', '%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/spark/python/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "/opt/spark/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/opt/spark/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
            "/opt/spark/python/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tip['date'] = transform_dates(tip, 'date', '%Y-%m-%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "review_small = review[review['date']>'2021-10-01']\n",
        "review_small.to_json('./data/review_small.json')\n",
        "review_big = review[review['date']<'2021-10-01']\n",
        "review_big.to_json('./data/review_big.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "tip_small = tip[tip['date']>'2021-10-01']\n",
        "tip_small.to_json('./data/tip_small.json')\n",
        "tip_big = tip[tip['date']<'2021-10-01']\n",
        "tip_big.to_json('./data/tip_big.json')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
